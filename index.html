<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
 <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
    crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut"
    crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k"
    crossorigin="anonymous"></script>
<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Nikos Zarifis | Minimal is a theme for GitHub Pages.</title>
<meta name="generator" content="Jekyll v3.8.7" />
<meta property="og:title" content="Nikos Zarifis" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Minimal is a theme for GitHub Pages." />
<meta property="og:description" content="Minimal is a theme for GitHub Pages." />
<link rel="canonical" href="http://localhost:4000/" />
<meta property="og:url" content="http://localhost:4000/" />
<meta property="og:site_name" content="Nikos Zarifis" />
<script type="application/ld+json">
{"url":"http://localhost:4000/","headline":"Nikos Zarifis","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/img/img1.jpeg"}},"name":"Nikos Zarifis","description":"Minimal is a theme for GitHub Pages.","@type":"WebSite","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <!-- <h1><a href="http://localhost:4000/">Nikos Zarifis</a></h1> -->
        
          <img src="/assets/img/img1.jpeg" alt="Logo"
          style="border-radius: 10px" />
        

        <h2>Nikos Zarifis</h2>
        <h3>zarifis [at] wisc.edu</h3>
        <p>Computer Sciences Department
University of Wisconsin–Madison</p>
        <!-- <p>Minimal is a theme for GitHub Pages.</p> -->

<!--          -->

<!--          -->

<!--          -->
<!--         <ul class="downloads"> -->
<!--           <li><a href="">Download <strong>ZIP File</strong></a></li> -->
<!--           <li><a href="">Download <strong>TAR Ball</strong></a></li> -->
<!--           <li><a href="">View On <strong>GitHub</strong></a></li> -->
<!--         </ul> -->
<!--          -->
      </header>
      <section>

      <p>I am a first year PhD student at the <a href="https://cs.wisc.edu">Computer Sciences
Department</a> of University of Wisconsin-Madison and a part
of the <a href="https://research.cs.wisc.edu/areas/theory/">Theory of Computing Group</a>.
I am very lucky to be advised by Professor <a href="http://iliasdiakonikolas.org">Ilias
Diakonikolas</a>. I completed my undergraduate studies
at the School of Electrical and Computer Engineering Department of the <a href="www.ntua.gr/en">National
Technical University of Athens</a> in Greece, where I was advised
by Professor <a href="http://www.softlab.ntua.gr/~fotakis/">Dimitris Fotakis</a>.</p>

<p>I am interested in algorithms and in theoretical machine learning. For more
information you can take a look in my <a href="./assets/img/cv.pdf">CV</a>.</p>

<h1 id="publications">Publications</h1>
<ol class="bibliography"><li>

<div id="DKZ20">
  
    <span class="title">Near-Optimal SQ Lower Bounds for Agnostically Learning Halfspaces and ReLUs under Gaussian Marginals <span class="nav">
    
     
    
    
  </span>
    </span>
    <span class="author">
      
        
          
            
              
                <a href="http://www.iliasdiakonikolas.org" target="_blank">  Ilias Diakonikolas</a>, 
              
            
          
        
      
        
          
            
              
                <a href="https://cseweb.ucsd.edu/~dakane" target="_blank">  Daniel M. Kane</a>, 
              
            
          
        
      
        
          
            
              and <em>Nikos Zarifis</em>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>Manuscript</em>
      
    </span>
  
<span id="DKZ20-materials">
 
    <!-- <li><a data-toggle="collapse" href="javascript:toggleDiv('DKZ20-bibtex')">BibTeX</a></li> -->
    <!-- <div id="DKZ20-bibtex" style="display:none;"> -->
    <!-- <pre class="collapse">@misc{DKZ20,
  author = {Diakonikolas, Ilias and Kane, Daniel M. and Zarifis, Nikos},
  title = {Near-Optimal SQ Lower Bounds for Agnostically Learning Halfspaces and ReLUs under Gaussian Marginals},
  note = {Manuscript},
  year = {2020}
}
</pre> -->
    <!-- </div> -->


  

</span>
  <!-- <span class="links"> -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- </span> -->

  <!-- <!-1- Hidden abstract block -1-> -->
  <!--  -->
</div>
</li>
<li>

<div id="diakonikolas2020learning">
  
    <span class="title">Learning Halfspaces with Tsybakov Noise <span class="nav">
    
    [<a data-toggle="collapse" href="#diakonikolas2020learning-abstract">abstract</a>]
    
     
    [<a href="http://arxiv.org/abs/2006.06467" target="_blank">arxiv</a>] 
    
    
    
  </span>
    </span>
    <span class="author">
      
        
          
            
              
                <a href="http://www.iliasdiakonikolas.org" target="_blank">  Ilias Diakonikolas</a>, 
              
            
          
        
      
        
          
            
              
                <a href="http://vkonton.github.io" target="_blank">  Vasilis Kontonis</a>, 
              
            
          
        
      
        
          
            
              
                <a href="http://www.tzamos.com" target="_blank">  Christos Tzamos</a>, 
              
            
          
        
      
        
          
            
              and <em>Nikos Zarifis</em>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>Manuscript</em>
      
    </span>
  
<span id="diakonikolas2020learning-materials">
 
    <!-- <li><a data-toggle="collapse" href="javascript:toggleDiv('diakonikolas2020learning-bibtex')">BibTeX</a></li> -->
    <!-- <div id="diakonikolas2020learning-bibtex" style="display:none;"> -->
    <!-- <pre class="collapse">@misc{diakonikolas2020learning,
  title = {Learning Halfspaces with Tsybakov Noise},
  author = {Diakonikolas, Ilias and Kontonis, Vasilis and Tzamos, Christos and Zarifis, Nikos},
  year = {2020},
  booktitle = {Manucript},
  eprint = {2006.06467},
  arxiv = {2006.06467},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG}
}
</pre> -->
    <!-- </div> -->


  
  <p id="diakonikolas2020learning-abstract" class="collapse" style="border: 1px dashed black;
  font-size:11px">We study the efficient PAC
  learnability of halfspaces in the presence of Tsybakov noise. In the Tsybakov
  noise model, each label is independently flipped with some probability which
  is controlled by an adversary. This noise model significantly generalizes the
  Massart noise model, by allowing the flipping probabilities to be arbitrarily
  close to 1/2 for a fraction of the samples. Our main result is the first
  non-trivial PAC learning algorithm for this problem under a broad family of
  structured distributions – satisfying certain concentration and
  (anti-)anti-concentration properties – including log-concave distributions.
  Specifically, we given an algorithm that achieves misclassification error
  εwith respect to the true halfspace, with quasi-polynomial runtime
  dependence in 1/ε. The only previous upper bound for this problem –
  even for the special case of log-concave distributions – was doubly
  exponential in 1/ε(and follows via the naive reduction to agnostic
  learning). Our approach relies on a novel computationally efficient procedure
  to certify whether a candidate solution is near-optimal, based on
  semi-definite programming. We use this certificate procedure as a black-box
  and turn it into an efficient learning algorithm by searching over the space
  of halfspaces via online convex optimization.</p>
  

</span>
  <!-- <span class="links"> -->
  <!--  -->
  <!--   [<a class="abstract">Abstract</a>] -->
  <!--  -->
  <!--  -->
  <!--   [<a href="http://arxiv.org/abs/2006.06467" target="_blank">arXiv</a>] -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- </span> -->

  <!-- <!-1- Hidden abstract block -1-> -->
  <!--  -->
  <!-- <span class="abstract hidden"> -->
  <!--   <p>We study the efficient PAC
  learnability of halfspaces in the presence of Tsybakov noise. In the Tsybakov
  noise model, each label is independently flipped with some probability which
  is controlled by an adversary. This noise model significantly generalizes the
  Massart noise model, by allowing the flipping probabilities to be arbitrarily
  close to 1/2 for a fraction of the samples. Our main result is the first
  non-trivial PAC learning algorithm for this problem under a broad family of
  structured distributions – satisfying certain concentration and
  (anti-)anti-concentration properties – including log-concave distributions.
  Specifically, we given an algorithm that achieves misclassification error
  εwith respect to the true halfspace, with quasi-polynomial runtime
  dependence in 1/ε. The only previous upper bound for this problem –
  even for the special case of log-concave distributions – was doubly
  exponential in 1/ε(and follows via the naive reduction to agnostic
  learning). Our approach relies on a novel computationally efficient procedure
  to certify whether a candidate solution is near-optimal, based on
  semi-definite programming. We use this certificate procedure as a black-box
  and turn it into an efficient learning algorithm by searching over the space
  of halfspaces via online convex optimization.</p> -->
  <!-- </span> -->
  <!--  -->
</div>
</li>
<li>

<div id="DKKZ20">
  
    <span class="title">Algorithms and SQ Lower Bounds for PAC Learning
	One-Hidden-Layer ReLU Networks <span class="nav">
    
    [<a data-toggle="collapse" href="#DKKZ20-abstract">abstract</a>]
    
     
    
    
  </span>
    </span>
    <span class="author">
      
        
          
            
              
                <a href="http://www.iliasdiakonikolas.org" target="_blank">  Ilias Diakonikolas</a>, 
              
            
          
        
      
        
          
            
              
                <a href="https://cseweb.ucsd.edu/~dakane" target="_blank">  Daniel M. Kane</a>, 
              
            
          
        
      
        
          
            
              
                <a href="http://vkonton.github.io" target="_blank">  Vasilis Kontonis</a>, 
              
            
          
        
      
        
          
            
              and <em>Nikos Zarifis</em>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the 33rd Annual Conference on Learning Theory (COLT 2020)</em>
    
    </span>
  
<span id="DKKZ20-materials">
 
    <!-- <li><a data-toggle="collapse" href="javascript:toggleDiv('DKKZ20-bibtex')">BibTeX</a></li> -->
    <!-- <div id="DKKZ20-bibtex" style="display:none;"> -->
    <!-- <pre class="collapse">@inproceedings{DKKZ20,
  author = {Diakonikolas, Ilias and Kane, Daniel M. and Kontonis, Vasilis and Zarifis, Nikos},
  title = {Algorithms and SQ Lower Bounds for PAC Learning
  	One-Hidden-Layer ReLU Networks},
  booktitle = {Proceedings of the 33rd Annual Conference on Learning Theory (COLT 2020)},
  year = {2020}
}
</pre> -->
    <!-- </div> -->


  
  <p id="DKKZ20-abstract" class="collapse" style="border: 1px dashed black;
  font-size:11px">We study the problem of PAC learning one-hidden-layer ReLU networks
with k hidden units
on \R^d under Gaussian marginals in the presence of additive label noise. 
For the case of positive coefficients, we give the first polynomial-time algorithm 
for this learning problem for k up to \tildeΩ(\sqrt\log d). 
Previously, no polynomial time algorithm was known, even for k=3.
This answers an open question posed by \citeKliv17. Importantly,
our algorithm does not require any assumptions about the rank of the weight matrix
and its complexity is independent of its condition number. On the negative side,
for the more general task of PAC learning one-hidden-layer ReLU networks with positive or negative coefficients, 
we prove a Statistical Query lower bound of d^Ω(k). Thus, we provide a 
separation between the two classes in terms of efficient learnability.
Our upper and lower bounds are general, extending to broader families of activation functions.

             </p>
  

</span>
  <!-- <span class="links"> -->
  <!--  -->
  <!--   [<a class="abstract">Abstract</a>] -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- </span> -->

  <!-- <!-1- Hidden abstract block -1-> -->
  <!--  -->
  <!-- <span class="abstract hidden"> -->
  <!--   <p>We study the problem of PAC learning one-hidden-layer ReLU networks
with k hidden units
on \R^d under Gaussian marginals in the presence of additive label noise. 
For the case of positive coefficients, we give the first polynomial-time algorithm 
for this learning problem for k up to \tildeΩ(\sqrt\log d). 
Previously, no polynomial time algorithm was known, even for k=3.
This answers an open question posed by \citeKliv17. Importantly,
our algorithm does not require any assumptions about the rank of the weight matrix
and its complexity is independent of its condition number. On the negative side,
for the more general task of PAC learning one-hidden-layer ReLU networks with positive or negative coefficients, 
we prove a Statistical Query lower bound of d^Ω(k). Thus, we provide a 
separation between the two classes in terms of efficient learnability.
Our upper and lower bounds are general, extending to broader families of activation functions.

             </p> -->
  <!-- </span> -->
  <!--  -->
</div>
</li>
<li>

<div id="diakonikolas2020learninh">
  
    <span class="title">Learning Halfspaces with Massart Noise Under Structured Distributions <span class="nav">
    
    [<a data-toggle="collapse" href="#diakonikolas2020learninh-abstract">abstract</a>]
    
     
    [<a href="http://arxiv.org/abs/2002.05632" target="_blank">arxiv</a>] 
    
    
    
  </span>
    </span>
    <span class="author">
      
        
          
            
              
                <a href="http://www.iliasdiakonikolas.org" target="_blank">  Ilias Diakonikolas</a>, 
              
            
          
        
      
        
          
            
              
                <a href="http://vkonton.github.io" target="_blank">  Vasilis Kontonis</a>, 
              
            
          
        
      
        
          
            
              
                <a href="http://www.tzamos.com" target="_blank">  Christos Tzamos</a>, 
              
            
          
        
      
        
          
            
              and <em>Nikos Zarifis</em>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the 33rd Annual Conference on Learning Theory (COLT 2020)</em>
    
    </span>
  
<span id="diakonikolas2020learninh-materials">
 
    <!-- <li><a data-toggle="collapse" href="javascript:toggleDiv('diakonikolas2020learninh-bibtex')">BibTeX</a></li> -->
    <!-- <div id="diakonikolas2020learninh-bibtex" style="display:none;"> -->
    <!-- <pre class="collapse">@inproceedings{diakonikolas2020learninh,
  title = {Learning Halfspaces with Massart Noise Under Structured Distributions},
  author = {Diakonikolas, Ilias and Kontonis, Vasilis and Tzamos, Christos and Zarifis, Nikos},
  year = {2020},
  booktitle = {Proceedings of the 33rd Annual Conference on Learning Theory (COLT 2020)},
  eprint = {2002.05632},
  arxiv = {2002.05632},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG}
}
</pre> -->
    <!-- </div> -->


  
  <p id="diakonikolas2020learninh-abstract" class="collapse" style="border: 1px dashed black;
  font-size:11px">We study the problem of learning
  halfspaces with Massart noise in the distribution-specific PAC model. We give
  the first computationally efficient algorithm for this problem with respect to
  a broad family of distributions, including log-concave distributions. This
  resolves an open question posed in a number of prior works. Our approach is
  extremely simple: We identify a smooth \em non-convex surrogate loss with
  the property that any approximate stationary point of this loss defines a
  halfspace that is close to the target halfspace. Given this structural result,
  we can use SGD to solve the underlying learning problem.</p>
  

</span>
  <!-- <span class="links"> -->
  <!--  -->
  <!--   [<a class="abstract">Abstract</a>] -->
  <!--  -->
  <!--  -->
  <!--   [<a href="http://arxiv.org/abs/2002.05632" target="_blank">arXiv</a>] -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- </span> -->

  <!-- <!-1- Hidden abstract block -1-> -->
  <!--  -->
  <!-- <span class="abstract hidden"> -->
  <!--   <p>We study the problem of learning
  halfspaces with Massart noise in the distribution-specific PAC model. We give
  the first computationally efficient algorithm for this problem with respect to
  a broad family of distributions, including log-concave distributions. This
  resolves an open question posed in a number of prior works. Our approach is
  extremely simple: We identify a smooth \em non-convex surrogate loss with
  the property that any approximate stationary point of this loss defines a
  halfspace that is close to the target halfspace. Given this structural result,
  we can use SGD to solve the underlying learning problem.</p> -->
  <!-- </span> -->
  <!--  -->
</div>
</li>
<li>

<div id="fotakis2019reallocating">
  
    <span class="title">Reallocating multiple facilities on the line <span class="nav">
    
    [<a data-toggle="collapse" href="#fotakis2019reallocating-abstract">abstract</a>]
    
     
    [<a href="http://arxiv.org/abs/1905.12379" target="_blank">arxiv</a>] 
    
    
    
  </span>
    </span>
    <span class="author">
      
        
          
            
              
                <a href="http://www.softlab.ntua.gr/~fotakis/" target="_blank">  Dimitris Fotakis</a>, 
              
            
          
        
      
        
          
            
              
              Loukas  Kavouras,
              
            
          
        
      
        
          
            
              
              Panagiotis  Kostopanagiotis,
              
            
          
        
      
        
          
            
              
              Philip  Lazos,
              
            
          
        
      
        
          
            
              
              Stratis  Skoulakis,
              
            
          
        
      
        
          
            
              and <em>Nikos Zarifis</em>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In  Proceedings of the 28th International Joint Conference on
  Artificial Intelligence (IJCAI-19)</em>
    
    </span>
  
<span id="fotakis2019reallocating-materials">
 
    <!-- <li><a data-toggle="collapse" href="javascript:toggleDiv('fotakis2019reallocating-bibtex')">BibTeX</a></li> -->
    <!-- <div id="fotakis2019reallocating-bibtex" style="display:none;"> -->
    <!-- <pre class="collapse">@inproceedings{fotakis2019reallocating,
  title = {Reallocating multiple facilities on the line},
  author = {Fotakis, Dimitris and Kavouras, Loukas and Kostopanagiotis, Panagiotis and Lazos, Philip and Skoulakis, Stratis and Zarifis, Nikos},
  booktitle = { Proceedings of the 28th International Joint Conference on
    Artificial Intelligence (IJCAI-19)},
  journal = {arXiv preprint},
  arxiv = {1905.12379},
  year = {2019}
}
</pre> -->
    <!-- </div> -->


  
  <p id="fotakis2019reallocating-abstract" class="collapse" style="border: 1px dashed black;
  font-size:11px">We study the multistage K-facility reallocation problem on the real line, where we maintain K facility locations over T stages, based on the stage-dependent locations of n agents. Each agent is connected to the nearest facility at each stage, and the facilities may move from one stage to another, to accommodate different agent locations. The objective is to minimize the connection cost of the agents plus the total moving cost of the facilities, over all stages. K-facility reallocation was introduced by de Keijzer and Wojtczak, where they mostly focused on the special case of a single facility. Using an LP-based approach, we present a polynomial time algorithm that computes the optimal solution for any number of facilities. We also consider online K-facility reallocation, where the algorithm becomes aware of agent locations in a stage-by-stage fashion. By exploiting an interesting connection to the classical K-server problem, we present a constant-competitive algorithm for K = 2 facilities.</p>
  

</span>
  <!-- <span class="links"> -->
  <!--  -->
  <!--   [<a class="abstract">Abstract</a>] -->
  <!--  -->
  <!--  -->
  <!--   [<a href="http://arxiv.org/abs/1905.12379" target="_blank">arXiv</a>] -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- </span> -->

  <!-- <!-1- Hidden abstract block -1-> -->
  <!--  -->
  <!-- <span class="abstract hidden"> -->
  <!--   <p>We study the multistage K-facility reallocation problem on the real line, where we maintain K facility locations over T stages, based on the stage-dependent locations of n agents. Each agent is connected to the nearest facility at each stage, and the facilities may move from one stage to another, to accommodate different agent locations. The objective is to minimize the connection cost of the agents plus the total moving cost of the facilities, over all stages. K-facility reallocation was introduced by de Keijzer and Wojtczak, where they mostly focused on the special case of a single facility. Using an LP-based approach, we present a polynomial time algorithm that computes the optimal solution for any number of facilities. We also consider online K-facility reallocation, where the algorithm becomes aware of agent locations in a stage-by-stage fashion. By exploiting an interesting connection to the classical K-server problem, we present a constant-competitive algorithm for K = 2 facilities.</p> -->
  <!-- </span> -->
  <!--  -->
</div>
</li></ol>


      </section>
      <!-- <footer> -->
      <!--    -->
      <!--   <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p> -->
      <!-- </footer> -->
    </div>
    <script src="/assets/js/scale.fix.js"></script>
    
  </body>
</html>
