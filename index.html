<!DOCTYPE html>
<html lang="en-US">

<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
    integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
    crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js"
    integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut"
    crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js"
    integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k"
    crossorigin="anonymous"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-20HH61PL7C"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-20HH61PL7C');
</script>
<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?family=Roboto&display=swap" rel="stylesheet">  
<script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Nikos Zarifis | Nikos Zarifis’ Homepage</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Nikos Zarifis" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Nikos Zarifis’ Homepage" />
<meta property="og:description" content="Nikos Zarifis’ Homepage" />
<link rel="canonical" href="http://localhost:4000/" />
<meta property="og:url" content="http://localhost:4000/" />
<meta property="og:site_name" content="Nikos Zarifis" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Nikos Zarifis" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","description":"Nikos Zarifis’ Homepage","headline":"Nikos Zarifis","name":"Nikos Zarifis","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/img/img5.jpg"}},"url":"http://localhost:4000/"}</script>
<!-- End Jekyll SEO tag -->

  <link rel="stylesheet" href="/assets/css/style.css?v=">
  <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
</head>

<body>
  <div style="display:none">
    \(
    \newcommand{\opt}{\mathrm{opt}}
    \newcommand{\eps}{\epsilon}
    \newcommand{\R}{\mathbb{R}}
\newcommand{\vec}{\mathbf}
	\newcommand{\wstar}{\w^{\ast}}
	\newcommand{\x}{\vec x}
	\newcommand{\w}{\vec w}
\newcommand{\wt}{\widetilde}
\newcommand{\wh}{\widehat}
\newcommand{\poly}{\mathrm{poly}}
\newcommand{\polylog}{\mathrm{polylog}}
\newcommand{\var}{\mathbf{Var}}
\newcommand{\cov}{\mathbf{Cov}}
    \)
  </div>
  <div class="wrapper">
    <header>
      <!-- <h1><a href="http://localhost:4000/">Nikos Zarifis</a></h1> -->
      
      <img src="/assets/img/img5.jpg" alt="Logo" style="border-radius: 10px" />
      

      <h2>Nikos Zarifis</h2>
      <h3>zarifis [at] wisc.edu</h3>
      <p>Computer Sciences Department
        University of Wisconsin–Madison</p>
      <!-- <p>Nikos Zarifis' Homepage</p> -->

      <!--          -->

      <!--          -->

      <!--          -->
      <!--         <ul class="downloads"> -->
      <!--           <li><a href="">Download <strong>ZIP File</strong></a></li> -->
      <!--           <li><a href="">Download <strong>TAR Ball</strong></a></li> -->
      <!--           <li><a href="">View On <strong>GitHub</strong></a></li> -->
      <!--         </ul> -->
      <!--          -->
    </header>
    <section>

      <p>I am a PhD student at the <a href="https://cs.wisc.edu">Computer Sciences
Department</a> of University of Wisconsin-Madison and a part
of the <a href="https://research.cs.wisc.edu/areas/theory/">Theory of Computing Group</a>.
I am very lucky to be advised by Professor <a href="http://iliasdiakonikolas.org">Ilias
Diakonikolas</a>. I completed my undergraduate studies
at the School of Electrical and Computer Engineering Department of the <a href="www.ntua.gr/en">National
Technical University of Athens</a> in Greece, where I was advised
by Professor <a href="http://www.softlab.ntua.gr/~fotakis/">Dimitris Fotakis</a>.</p>

<p>I am interested in algorithms and in theoretical machine learning. For more
information you can take a look in my <a href="./assets/img/cv.pdf">CV</a>.</p>

<h1 id="publications">Publications</h1>
<ol class="bibliography"><li>

<div id="DKTZ24">
  
    <span class="title">Online Learning of Halfspaces with Massart Noise <span class="nav">
    
    [<a data-toggle="collapse" href="#DKTZ24-abstract">abstract</a>]
    
     
    [<a href="http://arxiv.org/abs/2405.12958" target="_blank">arxiv</a>] 
    
    
    
  </span>
    </span>
    <span class="author">
      
      
       
       
       
          
            
              
                <a href="http://www.iliasdiakonikolas.org" target="_blank">  Ilias Diakonikolas</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="http://vkonton.github.io" target="_blank">  Vasilis Kontonis</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="http://www.tzamos.com" target="_blank">  Christos Tzamos</a>, 
              
            
          
        
      
      
       
       
       
          
            
              and <em>Nikos Zarifis</em>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>Manuscript</em>
      
 <!---   </span>
    <span class="notes"> -->
    
      
      </span>
  

<span id="DKTZ24-materials">
 
    <!-- <li><a data-toggle="collapse" href="javascript:toggleDiv('DKTZ24-bibtex')">BibTeX</a></li> -->
    <!-- <div id="DKTZ24-bibtex" style="display:none;"> -->
    <!-- <pre class="collapse">@misc{DKTZ24,
  title = {Online Learning of Halfspaces with Massart Noise},
  author = {Diakonikolas, Ilias and Kontonis, Vasilis and Tzamos, Christos and Zarifis, Nikos},
  year = {2024},
  arxiv = {2405.12958},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG}
}
</pre> -->
    <!-- </div> -->


  
  <p id="DKTZ24-abstract" class="collapse" style="border: 1px dashed
         black;padding:2px;
  font-size:11px">We study the task of online learning in the presence of Massart noise. Instead of assuming that the online adversary chooses an arbitrary sequence of labels, we assume that the context $\mathbf{x}$ is selected adversarially but the label $y$ presented to the learner disagrees with the ground-truth label of $\mathbf{x}$ with unknown probability at most $\eta$. We study the fundamental class of $\gamma$-margin linear classifiers and present a computationally efficient algorithm that achieves mistake bound $\eta T + o(T)$. Our mistake bound is qualitatively tight for efficient algorithms: it is known that even in the offline setting achieving classification error better than $\eta$ requires super-polynomial time in the SQ model.
We extend our online learning model to a $k$-arm contextual bandit setting where the rewards -- instead of satisfying commonly used realizability assumptions -- are consistent (in expectation) with some linear ranking function with weight vector $\mathbf{w}^\ast$. Given a list of contexts $\mathbf{x}_1,\ldots \mathbf{x}_k$, if $\mathbf{w}^*\cdot \mathbf{x}_i &gt; \mathbf{w}^* \cdot \mathbf{x}_j$, the expected reward of action $i$ must be larger than that of $j$ by at least $\Delta$. We use our Massart online learner to design an efficient bandit algorithm that obtains expected reward at least $(1-1/k)~ \Delta T - o(T)$ bigger than choosing a random action at every round.</p>
  

</span>
  <!-- <span class="links"> -->
  <!--  -->
  <!--   [<a class="abstract">Abstract</a>] -->
  <!--  -->
  <!--  -->
  <!--   [<a href="http://arxiv.org/abs/2405.12958"
    target="_blank">arXiv</a>] -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- </span> -->

  <!-- <!-1- Hidden abstract block -1-> -->
  <!--  -->
  <!-- <span class="abstract hidden"> -->
  <!--   <p>We study the task of online learning in the presence of Massart noise. Instead of assuming that the online adversary chooses an arbitrary sequence of labels, we assume that the context $\mathbf{x}$ is selected adversarially but the label $y$ presented to the learner disagrees with the ground-truth label of $\mathbf{x}$ with unknown probability at most $\eta$. We study the fundamental class of $\gamma$-margin linear classifiers and present a computationally efficient algorithm that achieves mistake bound $\eta T + o(T)$. Our mistake bound is qualitatively tight for efficient algorithms: it is known that even in the offline setting achieving classification error better than $\eta$ requires super-polynomial time in the SQ model.
We extend our online learning model to a $k$-arm contextual bandit setting where the rewards -- instead of satisfying commonly used realizability assumptions -- are consistent (in expectation) with some linear ranking function with weight vector $\mathbf{w}^\ast$. Given a list of contexts $\mathbf{x}_1,\ldots \mathbf{x}_k$, if $\mathbf{w}^*\cdot \mathbf{x}_i > \mathbf{w}^* \cdot \mathbf{x}_j$, the expected reward of action $i$ must be larger than that of $j$ by at least $\Delta$. We use our Massart online learner to design an efficient bandit algorithm that obtains expected reward at least $(1-1/k)~ \Delta T - o(T)$ bigger than choosing a random action at every round.</p> -->
  <!-- </span> -->
  <!--  -->
</div>
</li>
<li>

<div id="DZ24">
  
    <span class="title">A Near-optimal Algorithm for Learning Margin Halfspaces with Massart Noise <span class="nav">
    
     
    
    
  </span>
    </span>
    <span class="author">
      
      
       
       
       
          
            
              
                <a href="http://www.iliasdiakonikolas.org" target="_blank">  Ilias Diakonikolas</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
              Lisheng  Ren,
              
            
          
        
      
      
       
       
       
          
            
              and <em>Nikos Zarifis</em>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Advances in Neural Information Processing Systems (NeurIPS 2024)</em>
    
 <!---   </span>
    <span class="notes"> -->
    
      
      <small>  * Spotlight</small>
        
      </span>
  

<span id="DZ24-materials">
 
    <!-- <li><a data-toggle="collapse" href="javascript:toggleDiv('DZ24-bibtex')">BibTeX</a></li> -->
    <!-- <div id="DZ24-bibtex" style="display:none;"> -->
    <!-- <pre class="collapse">@inproceedings{DZ24,
  title = {A Near-optimal Algorithm for Learning Margin Halfspaces with Massart Noise},
  author = {Diakonikolas, Ilias and Ren, Lisheng and Zarifis, Nikos},
  year = {2024},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS 2024)},
  notes = {* Spotlight}
}
</pre> -->
    <!-- </div> -->


  

</span>
  <!-- <span class="links"> -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- </span> -->

  <!-- <!-1- Hidden abstract block -1-> -->
  <!--  -->
</div>
</li>
<li>

<div id="DRZ24">
  
    <span class="title">Reliable Learning of Halfspaces under Gaussian Marginals <span class="nav">
    
     
    
    
  </span>
    </span>
    <span class="author">
      
      
       
       
       
          
            
              
                <a href="http://www.iliasdiakonikolas.org" target="_blank">  Ilias Diakonikolas</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
              Lisheng  Ren,
              
            
          
        
      
      
       
       
       
          
            
              and <em>Nikos Zarifis</em>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Advances in Neural Information Processing Systems (NeurIPS 2024)</em>
    
 <!---   </span>
    <span class="notes"> -->
    
      
      <small>  * Spotlight</small>
        
      </span>
  

<span id="DRZ24-materials">
 
    <!-- <li><a data-toggle="collapse" href="javascript:toggleDiv('DRZ24-bibtex')">BibTeX</a></li> -->
    <!-- <div id="DRZ24-bibtex" style="display:none;"> -->
    <!-- <pre class="collapse">@inproceedings{DRZ24,
  title = {Reliable Learning of Halfspaces under Gaussian Marginals},
  author = {Diakonikolas, Ilias and Ren, Lisheng and Zarifis, Nikos},
  year = {2024},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS 2024)},
  notes = {* Spotlight}
}
</pre> -->
    <!-- </div> -->


  

</span>
  <!-- <span class="links"> -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- </span> -->

  <!-- <!-1- Hidden abstract block -1-> -->
  <!--  -->
</div>
</li>
<li>

<div id="WZDD24">
  
    <span class="title">Sample and Computationally Efficient Robust Learning of Gaussian Single-Index Models <span class="nav">
    
     
    
    
  </span>
    </span>
    <span class="author">
      
      
       
       
       
          
            
              
              Puqian  Wang*,
              
            
          
        
      
      
       
       
       
          
            
              <em>Nikos Zarifis*</em>,
            
          
        
      
      
       
       
       
          
            
              
                <a href="http://www.iliasdiakonikolas.org" target="_blank">  Ilias Diakonikolas</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                and <a href="https://www.jelena-diakonikolas.com" target="_blank">Jelena Diakonikolas</a> 
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Advances in Neural Information Processing Systems (NeurIPS 2024)</em>
    
 <!---   </span>
    <span class="notes"> -->
    
      
      <small>  * Equal contribution</small>
        
      </span>
  

<span id="WZDD24-materials">
 
    <!-- <li><a data-toggle="collapse" href="javascript:toggleDiv('WZDD24-bibtex')">BibTeX</a></li> -->
    <!-- <div id="WZDD24-bibtex" style="display:none;"> -->
    <!-- <pre class="collapse">@inproceedings{WZDD24,
  title = {Sample and Computationally Efficient Robust Learning of Gaussian Single-Index Models},
  author = {Wang*, Puqian and Zarifis*, Nikos and Diakonikolas, Ilias and Diakonikolas, Jelena},
  year = {2024},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS 2024)},
  notes = {* Equal contribution}
}
</pre> -->
    <!-- </div> -->


  

</span>
  <!-- <span class="links"> -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- </span> -->

  <!-- <!-1- Hidden abstract block -1-> -->
  <!--  -->
</div>
</li>
<li>

<div id="ZWDD24">
  
    <span class="title">Robustly Learning Single-Index Models via Alignment Sharpness <span class="nav">
    
    [<a data-toggle="collapse" href="#ZWDD24-abstract">abstract</a>]
    
     
    [<a href="http://arxiv.org/abs/2402.17756" target="_blank">arxiv</a>] 
    
    
    
  </span>
    </span>
    <span class="author">
      
      
       
       
       
          
            
              <em>Nikos Zarifis*</em>,
            
          
        
      
      
       
       
       
          
            
              
              Puqian  Wang*,
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="http://www.iliasdiakonikolas.org" target="_blank">  Ilias Diakonikolas</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                and <a href="https://www.jelena-diakonikolas.com" target="_blank">Jelena Diakonikolas</a> 
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the 41st International Conference on Machine
  Learning (ICML 2024)</em>
    
 <!---   </span>
    <span class="notes"> -->
    
      
      <small>  * Equal contribution</small>
        
      </span>
  

<span id="ZWDD24-materials">
 
    <!-- <li><a data-toggle="collapse" href="javascript:toggleDiv('ZWDD24-bibtex')">BibTeX</a></li> -->
    <!-- <div id="ZWDD24-bibtex" style="display:none;"> -->
    <!-- <pre class="collapse">@inproceedings{ZWDD24,
  title = {Robustly Learning Single-Index Models via Alignment Sharpness},
  author = {Zarifis*, Nikos and Wang*, Puqian and Diakonikolas, Ilias and Diakonikolas, Jelena},
  year = {2024},
  arxiv = {2402.17756},
  booktitle = {Proceedings of the 41st International Conference on Machine
    Learning (ICML 2024)},
  notes = {* Equal contribution}
}
</pre> -->
    <!-- </div> -->


  
  <p id="ZWDD24-abstract" class="collapse" style="border: 1px dashed
         black;padding:2px;
  font-size:11px">We study the problem of learning Single-Index Models under the
           $L_2^2$ loss in the agnostic model. We give an efficient learning
           algorithm, achieving a constant factor approximation to the optimal
           loss, that succeeds under a range of distributions (including
           log-concave distributions) and a broad class of monotone and
           Lipschitz link functions. This is the first efficient constant factor
           approximate agnostic learner, even for Gaussian data and for any
           nontrivial class of link functions. Prior work for the case of
           unknown link function either works in the realizable setting or does
           not attain constant factor approximation. The main technical
           ingredient enabling our algorithm and analysis is a novel notion of a
           local error bound in optimization that we term alignment sharpness
           and that may be of broader interest.</p>
  

</span>
  <!-- <span class="links"> -->
  <!--  -->
  <!--   [<a class="abstract">Abstract</a>] -->
  <!--  -->
  <!--  -->
  <!--   [<a href="http://arxiv.org/abs/2402.17756"
    target="_blank">arXiv</a>] -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- </span> -->

  <!-- <!-1- Hidden abstract block -1-> -->
  <!--  -->
  <!-- <span class="abstract hidden"> -->
  <!--   <p>We study the problem of learning Single-Index Models under the
           $L_2^2$ loss in the agnostic model. We give an efficient learning
           algorithm, achieving a constant factor approximation to the optimal
           loss, that succeeds under a range of distributions (including
           log-concave distributions) and a broad class of monotone and
           Lipschitz link functions. This is the first efficient constant factor
           approximate agnostic learner, even for Gaussian data and for any
           nontrivial class of link functions. Prior work for the case of
           unknown link function either works in the realizable setting or does
           not attain constant factor approximation. The main technical
           ingredient enabling our algorithm and analysis is a novel notion of a
           local error bound in optimization that we term alignment sharpness
           and that may be of broader interest.</p> -->
  <!-- </span> -->
  <!--  -->
</div>
</li>
<li>

<div id="DKLZ24">
  
    <span class="title">Testable Learning of General Halfspaces with Adversarial Label Noise <span class="nav">
    
     
    
    
  </span>
    </span>
    <span class="author">
      
      
       
       
       
          
            
              
                <a href="http://www.iliasdiakonikolas.org" target="_blank">  Ilias Diakonikolas</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="https://cseweb.ucsd.edu/~dakane" target="_blank">  Daniel M. Kane</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="https://lteins.github.io" target="_blank">  Sihan Liu</a>, 
              
            
          
        
      
      
       
       
       
          
            
              and <em>Nikos Zarifis</em>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the 37th Annual Conference on Learning Theory
  (COLT 2024)</em>
    
 <!---   </span>
    <span class="notes"> -->
    
      
      </span>
  

<span id="DKLZ24-materials">
 
    <!-- <li><a data-toggle="collapse" href="javascript:toggleDiv('DKLZ24-bibtex')">BibTeX</a></li> -->
    <!-- <div id="DKLZ24-bibtex" style="display:none;"> -->
    <!-- <pre class="collapse">@inproceedings{DKLZ24,
  title = {Testable Learning of General Halfspaces with Adversarial Label Noise},
  author = {Diakonikolas, Ilias and Kane, Daniel M. and Liu, Sihan and Zarifis, Nikos},
  year = {2024},
  booktitle = {Proceedings of the 37th Annual Conference on Learning Theory
    (COLT 2024)},
  primaryclass = {cs.LG}
}
</pre> -->
    <!-- </div> -->


  

</span>
  <!-- <span class="links"> -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- </span> -->

  <!-- <!-1- Hidden abstract block -1-> -->
  <!--  -->
</div>
</li>
<li>

<div id="DKPZ24">
  
    <span class="title">Statistical Query Lower Bounds for Learning Truncated Gaussians <span class="nav">
    
    [<a data-toggle="collapse" href="#DKPZ24-abstract">abstract</a>]
    
     
    [<a href="http://arxiv.org/abs/2403.02300" target="_blank">arxiv</a>] 
    
    
    
  </span>
    </span>
    <span class="author">
      
      
       
       
       
          
            
              
                <a href="http://www.iliasdiakonikolas.org" target="_blank">  Ilias Diakonikolas</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="https://cseweb.ucsd.edu/~dakane" target="_blank">  Daniel M. Kane</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="https://thanasispittas.github.io" target="_blank">  Thanasis Pittas</a>, 
              
            
          
        
      
      
       
       
       
          
            
              and <em>Nikos Zarifis</em>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the 37th Annual Conference on Learning Theory
  (COLT 2024)</em>
    
 <!---   </span>
    <span class="notes"> -->
    
      
      </span>
  

<span id="DKPZ24-materials">
 
    <!-- <li><a data-toggle="collapse" href="javascript:toggleDiv('DKPZ24-bibtex')">BibTeX</a></li> -->
    <!-- <div id="DKPZ24-bibtex" style="display:none;"> -->
    <!-- <pre class="collapse">@inproceedings{DKPZ24,
  title = {Statistical Query Lower Bounds for Learning Truncated Gaussians},
  author = {Diakonikolas, Ilias and Kane, Daniel M. and Pittas, Thanasis and Zarifis, Nikos},
  year = {2024},
  arxiv = {2403.02300},
  booktitle = {Proceedings of the 37th Annual Conference on Learning Theory
    (COLT 2024)},
  primaryclass = {cs.LG}
}
</pre> -->
    <!-- </div> -->


  
  <p id="DKPZ24-abstract" class="collapse" style="border: 1px dashed
         black;padding:2px;
  font-size:11px">We study the problem of estimating the mean of an identity covariance
          Gaussian in the truncated setting, in the regime when the truncation
          set comes from a low-complexity family $\mathcal{C}$ of sets.
          Specifically, for a fixed but unknown truncation set $S \subseteq
          \mathbb{R}^d$, we are given access to samples from the distribution
          $\mathcal{N}(\boldsymbol{ \mu}, \mathbf{ I})$ truncated to the set
          $S$. The goal is to estimate $\boldsymbol\mu$ within accuracy
          $\epsilon&gt;0$ in $\ell_2$-norm. Our main result is a Statistical Query
          (SQ) lower bound suggesting a super-polynomial information-computation
          gap for this task. In more detail, we show that the complexity of any
          SQ algorithm for this problem is $d^{\mathrm{poly}(1/\epsilon)}$, even
          when the class $\mathcal{C}$ is simple so that
          $\mathrm{poly}(d/\epsilon)$ samples information-theoretically suffice.
          Concretely, our SQ lower bound applies when $\mathcal{C}$ is a union
          of a bounded number of rectangles whose VC dimension and Gaussian
          surface are small. As a corollary of our construction, it also follows
          that the complexity of the previously known algorithm for this task is
          qualitatively best possible.</p>
  

</span>
  <!-- <span class="links"> -->
  <!--  -->
  <!--   [<a class="abstract">Abstract</a>] -->
  <!--  -->
  <!--  -->
  <!--   [<a href="http://arxiv.org/abs/2403.02300"
    target="_blank">arXiv</a>] -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- </span> -->

  <!-- <!-1- Hidden abstract block -1-> -->
  <!--  -->
  <!-- <span class="abstract hidden"> -->
  <!--   <p>We study the problem of estimating the mean of an identity covariance
          Gaussian in the truncated setting, in the regime when the truncation
          set comes from a low-complexity family $\mathcal{C}$ of sets.
          Specifically, for a fixed but unknown truncation set $S \subseteq
          \mathbb{R}^d$, we are given access to samples from the distribution
          $\mathcal{N}(\boldsymbol{ \mu}, \mathbf{ I})$ truncated to the set
          $S$. The goal is to estimate $\boldsymbol\mu$ within accuracy
          $\epsilon>0$ in $\ell_2$-norm. Our main result is a Statistical Query
          (SQ) lower bound suggesting a super-polynomial information-computation
          gap for this task. In more detail, we show that the complexity of any
          SQ algorithm for this problem is $d^{\mathrm{poly}(1/\epsilon)}$, even
          when the class $\mathcal{C}$ is simple so that
          $\mathrm{poly}(d/\epsilon)$ samples information-theoretically suffice.
          Concretely, our SQ lower bound applies when $\mathcal{C}$ is a union
          of a bounded number of rectangles whose VC dimension and Gaussian
          surface are small. As a corollary of our construction, it also follows
          that the complexity of the previously known algorithm for this task is
          qualitatively best possible.</p> -->
  <!-- </span> -->
  <!--  -->
</div>
</li>
<li>

<div id="DKKTZ23">
  
    <span class="title">Agnostically Learning Multi-index Models with Queries <span class="nav">
    
    [<a data-toggle="collapse" href="#DKKTZ23-abstract">abstract</a>]
    
     
    [<a href="http://arxiv.org/abs/2312.16616" target="_blank">arxiv</a>] 
    
    
    
  </span>
    </span>
    <span class="author">
      
      
       
       
       
          
            
              
                <a href="http://www.iliasdiakonikolas.org" target="_blank">  Ilias Diakonikolas</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="https://cseweb.ucsd.edu/~dakane" target="_blank">  Daniel M. Kane</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="http://vkonton.github.io" target="_blank">  Vasilis Kontonis</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="http://www.tzamos.com" target="_blank">  Christos Tzamos</a>, 
              
            
          
        
      
      
       
       
       
          
            
              and <em>Nikos Zarifis</em>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>Manuscript</em>
      
 <!---   </span>
    <span class="notes"> -->
    
      
      </span>
  

<span id="DKKTZ23-materials">
 
    <!-- <li><a data-toggle="collapse" href="javascript:toggleDiv('DKKTZ23-bibtex')">BibTeX</a></li> -->
    <!-- <div id="DKKTZ23-bibtex" style="display:none;"> -->
    <!-- <pre class="collapse">@misc{DKKTZ23,
  title = {Agnostically Learning Multi-index Models with Queries},
  author = {Diakonikolas, Ilias and Kane, Daniel M. and Kontonis, Vasilis and Tzamos, Christos and Zarifis, Nikos},
  year = {2023},
  arxiv = {2312.16616},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG}
}
</pre> -->
    <!-- </div> -->


  
  <p id="DKKTZ23-abstract" class="collapse" style="border: 1px dashed
         black;padding:2px;
  font-size:11px">We study the power of query access for the task of agnostic learning under the Gaussian distribution. In the agnostic model, no assumptions are made on the labels and the goal is to compute a hypothesis that is competitive with the {\em best-fit} function in a known class, i.e., it achieves error $\mathrm{opt}+\epsilon$, where $\mathrm{opt}$ is the error of the best function in the class. We focus on a general family of Multi-Index Models (MIMs), which are $d$-variate functions that depend only on few relevant directions, i.e., have the form $g(\mathbf{W} \mathbf{x})$ for an unknown link function $g$ and a $k \times d$ matrix $\mathbf{W}$. Multi-index models cover a wide range of commonly studied function classes, including constant-depth neural networks with ReLU activations, and intersections of halfspaces.
Our main result shows that query access gives significant runtime improvements over random examples for agnostically learning MIMs. Under standard regularity assumptions for the link function (namely, bounded variation or surface area), we give an agnostic query learner for MIMs with complexity $O(k)^{\mathrm{poly}(1/\epsilon)} \; \mathrm{poly}(d) $. In contrast, algorithms that rely only on random examples inherently require $d^{\mathrm{poly}(1/\epsilon)}$ samples and runtime, even for the basic problem of agnostically learning a single ReLU or a halfspace.
Our algorithmic result establishes a strong computational separation between the agnostic PAC and the agnostic PAC+Query models under the Gaussian distribution. Prior to our work, no such separation was known -- even for the special case of agnostically learning a single halfspace, for which it was an open problem first posed by Feldman. Our results are enabled by a general dimension-reduction technique that leverages query access to estimate gradients of (a smoothed version of) the underlying label function.</p>
  

</span>
  <!-- <span class="links"> -->
  <!--  -->
  <!--   [<a class="abstract">Abstract</a>] -->
  <!--  -->
  <!--  -->
  <!--   [<a href="http://arxiv.org/abs/2312.16616"
    target="_blank">arXiv</a>] -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- </span> -->

  <!-- <!-1- Hidden abstract block -1-> -->
  <!--  -->
  <!-- <span class="abstract hidden"> -->
  <!--   <p>We study the power of query access for the task of agnostic learning under the Gaussian distribution. In the agnostic model, no assumptions are made on the labels and the goal is to compute a hypothesis that is competitive with the {\em best-fit} function in a known class, i.e., it achieves error $\mathrm{opt}+\epsilon$, where $\mathrm{opt}$ is the error of the best function in the class. We focus on a general family of Multi-Index Models (MIMs), which are $d$-variate functions that depend only on few relevant directions, i.e., have the form $g(\mathbf{W} \mathbf{x})$ for an unknown link function $g$ and a $k \times d$ matrix $\mathbf{W}$. Multi-index models cover a wide range of commonly studied function classes, including constant-depth neural networks with ReLU activations, and intersections of halfspaces.
Our main result shows that query access gives significant runtime improvements over random examples for agnostically learning MIMs. Under standard regularity assumptions for the link function (namely, bounded variation or surface area), we give an agnostic query learner for MIMs with complexity $O(k)^{\mathrm{poly}(1/\epsilon)} \; \mathrm{poly}(d) $. In contrast, algorithms that rely only on random examples inherently require $d^{\mathrm{poly}(1/\epsilon)}$ samples and runtime, even for the basic problem of agnostically learning a single ReLU or a halfspace.
Our algorithmic result establishes a strong computational separation between the agnostic PAC and the agnostic PAC+Query models under the Gaussian distribution. Prior to our work, no such separation was known -- even for the special case of agnostically learning a single halfspace, for which it was an open problem first posed by Feldman. Our results are enabled by a general dimension-reduction technique that leverages query access to estimate gradients of (a smoothed version of) the underlying label function.</p> -->
  <!-- </span> -->
  <!--  -->
</div>
</li>
<li>

<div id="DKKLZ24">
  
    <span class="title">Super Non-singular Decompositions of Polynomials and their Application to Robustly Learning Low-degree PTFs <span class="nav">
    
    [<a data-toggle="collapse" href="#DKKLZ24-abstract">abstract</a>]
    
     
    
    
  </span>
    </span>
    <span class="author">
      
      
       
       
       
          
            
              
                <a href="http://www.iliasdiakonikolas.org" target="_blank">  Ilias Diakonikolas</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="https://cseweb.ucsd.edu/~dakane" target="_blank">  Daniel M. Kane</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="http://vkonton.github.io" target="_blank">  Vasilis Kontonis</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="https://lteins.github.io" target="_blank">  Sihan Liu</a>, 
              
            
          
        
      
      
       
       
       
          
            
              and <em>Nikos Zarifis</em>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the 56th Annual ACM Symposium on Theory of
  Computing (STOC 2024)</em>
    
 <!---   </span>
    <span class="notes"> -->
    
      
      </span>
  

<span id="DKKLZ24-materials">
 
    <!-- <li><a data-toggle="collapse" href="javascript:toggleDiv('DKKLZ24-bibtex')">BibTeX</a></li> -->
    <!-- <div id="DKKLZ24-bibtex" style="display:none;"> -->
    <!-- <pre class="collapse">@inproceedings{DKKLZ24,
  title = {Super Non-singular Decompositions of Polynomials and their Application to Robustly Learning Low-degree PTFs},
  author = {Diakonikolas, Ilias and Kane, Daniel M. and Kontonis, Vasilis and Liu, Sihan and Zarifis, Nikos},
  year = {2024},
  booktitle = {Proceedings of the 56th Annual ACM Symposium on Theory of
    Computing (STOC 2024)}
}
</pre> -->
    <!-- </div> -->


  
  <p id="DKKLZ24-abstract" class="collapse" style="border: 1px dashed
         black;padding:2px;
  font-size:11px">We study the efficient learnability of low-degree polynomial threshold functions
(PTFs) in the presence of a constant fraction of adversarial corruptions. 
Our main algorithmic result is a polynomial-time PAC learning algorithm for this 
concept class in the strong contamination model under the Gaussian distribution 
with error guarantee $O_{d, c}(\opt^{1-c})$, for any desired constant $c&gt;0$, 
where $\opt$ is the fraction of corruptions. 
In the strong contamination model, an omniscient adversary 
can arbitrarily corrupt an $\opt$-fraction 
of the data points and their labels. 
This model generalizes the malicious noise model and the adversarial label noise 
model. Prior to our work, known polynomial-time algorithms 
in this corruption model (or even in the weaker adversarial label noise model) 
achieved error $\tilde{O}_d(\opt^{1/(d+1)})$, which 
deteriorates significantly as a function of the degree $d$. 

Our algorithm employs an iterative approach inspired by localization techniques 
previously used in the context of learning linear threshold functions. 
Specifically, we use a robust perceptron algorithm to compute 
a good partial classifier and then iterate on the unclassified points. 
In order to achieve this, we need to take a set defined by a number 
of polynomial inequalities and partition it into several well-behaved subsets. 
To this end, we develop new polynomial decomposition techniques 
that may be of independent interest.</p>
  

</span>
  <!-- <span class="links"> -->
  <!--  -->
  <!--   [<a class="abstract">Abstract</a>] -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- </span> -->

  <!-- <!-1- Hidden abstract block -1-> -->
  <!--  -->
  <!-- <span class="abstract hidden"> -->
  <!--   <p>We study the efficient learnability of low-degree polynomial threshold functions
(PTFs) in the presence of a constant fraction of adversarial corruptions. 
Our main algorithmic result is a polynomial-time PAC learning algorithm for this 
concept class in the strong contamination model under the Gaussian distribution 
with error guarantee $O_{d, c}(\opt^{1-c})$, for any desired constant $c>0$, 
where $\opt$ is the fraction of corruptions. 
In the strong contamination model, an omniscient adversary 
can arbitrarily corrupt an $\opt$-fraction 
of the data points and their labels. 
This model generalizes the malicious noise model and the adversarial label noise 
model. Prior to our work, known polynomial-time algorithms 
in this corruption model (or even in the weaker adversarial label noise model) 
achieved error $\tilde{O}_d(\opt^{1/(d+1)})$, which 
deteriorates significantly as a function of the degree $d$. 

Our algorithm employs an iterative approach inspired by localization techniques 
previously used in the context of learning linear threshold functions. 
Specifically, we use a robust perceptron algorithm to compute 
a good partial classifier and then iterate on the unclassified points. 
In order to achieve this, we need to take a set defined by a number 
of polynomial inequalities and partition it into several well-behaved subsets. 
To this end, we develop new polynomial decomposition techniques 
that may be of independent interest.</p> -->
  <!-- </span> -->
  <!--  -->
</div>
</li>
<li>

<div id="DDKWZ23b">
  
    <span class="title">Near-Optimal Bounds for Learning Gaussian Halfspaces with Random Classification Noise <span class="nav">
    
    [<a data-toggle="collapse" href="#DDKWZ23b-abstract">abstract</a>]
    
     
    [<a href="http://arxiv.org/abs/2307.08438" target="_blank">arxiv</a>] 
    
    
    
  </span>
    </span>
    <span class="author">
      
      
       
       
       
          
            
              
                <a href="http://www.iliasdiakonikolas.org" target="_blank">  Ilias Diakonikolas</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="https://www.jelena-diakonikolas.com" target="_blank">  Jelena Diakonikolas</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="https://cseweb.ucsd.edu/~dakane" target="_blank">  Daniel M. Kane</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="https://puqianwang.github.io" target="_blank">  Puqian Wang</a>, 
              
            
          
        
      
      
       
       
       
          
            
              and <em>Nikos Zarifis</em>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Advances in Neural Information Processing Systems (NeurIPS
  2023)</em>
    
 <!---   </span>
    <span class="notes"> -->
    
      
      </span>
  

<span id="DDKWZ23b-materials">
 
    <!-- <li><a data-toggle="collapse" href="javascript:toggleDiv('DDKWZ23b-bibtex')">BibTeX</a></li> -->
    <!-- <div id="DDKWZ23b-bibtex" style="display:none;"> -->
    <!-- <pre class="collapse">@inproceedings{DDKWZ23b,
  title = {Near-Optimal Bounds for Learning Gaussian Halfspaces with Random Classification Noise},
  author = {Diakonikolas, Ilias and Diakonikolas, Jelena and Kane, Daniel M. and Wang, Puqian and Zarifis, Nikos},
  year = {2023},
  arxiv = {2307.08438},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS
    2023)}
}
</pre> -->
    <!-- </div> -->


  
  <p id="DDKWZ23b-abstract" class="collapse" style="border: 1px dashed
         black;padding:2px;
  font-size:11px">We study the problem of learning general (i.e., not necessarily
           homogeneous) halfspaces with Random Classification Noise under the
           Gaussian distribution. We establish nearly-matching algorithmic and
           Statistical Query (SQ) lower bound results revealing a surprising
           information-computation gap for this basic problem. Specifically, the
           sample complexity of this learning problem is
           $\widetilde{\Theta}(d/\epsilon)$, where $d$ is the dimension and
           $\epsilon$ is the excess error. Our positive result is a
           computationally efficient learning algorithm with sample complexity
           $\tilde{O}(d/\epsilon + d/(\max\{p, \epsilon\})^2)$, where $p$
           quantifies the bias of the target halfspace. On the lower bound side,
           we show that any efficient SQ algorithm (or low-degree test) for the
           problem requires sample complexity at least $\Omega(d^{1/2}/(\max\{p,
           \epsilon\})^2)$. Our lower bound suggests that this quadratic
  dependence on $1/\epsilon$ is inherent for efficient algorithms.</p>
  

</span>
  <!-- <span class="links"> -->
  <!--  -->
  <!--   [<a class="abstract">Abstract</a>] -->
  <!--  -->
  <!--  -->
  <!--   [<a href="http://arxiv.org/abs/2307.08438"
    target="_blank">arXiv</a>] -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- </span> -->

  <!-- <!-1- Hidden abstract block -1-> -->
  <!--  -->
  <!-- <span class="abstract hidden"> -->
  <!--   <p>We study the problem of learning general (i.e., not necessarily
           homogeneous) halfspaces with Random Classification Noise under the
           Gaussian distribution. We establish nearly-matching algorithmic and
           Statistical Query (SQ) lower bound results revealing a surprising
           information-computation gap for this basic problem. Specifically, the
           sample complexity of this learning problem is
           $\widetilde{\Theta}(d/\epsilon)$, where $d$ is the dimension and
           $\epsilon$ is the excess error. Our positive result is a
           computationally efficient learning algorithm with sample complexity
           $\tilde{O}(d/\epsilon + d/(\max\{p, \epsilon\})^2)$, where $p$
           quantifies the bias of the target halfspace. On the lower bound side,
           we show that any efficient SQ algorithm (or low-degree test) for the
           problem requires sample complexity at least $\Omega(d^{1/2}/(\max\{p,
           \epsilon\})^2)$. Our lower bound suggests that this quadratic
  dependence on $1/\epsilon$ is inherent for efficient algorithms.</p> -->
  <!-- </span> -->
  <!--  -->
</div>
</li>
<li>

<div id="DKKLZ23">
  
    <span class="title">Efficient Testable Learning of Halfspaces with Adversarial Label Noise <span class="nav">
    
    [<a data-toggle="collapse" href="#DKKLZ23-abstract">abstract</a>]
    
     
    [<a href="http://arxiv.org/abs/2303.05485" target="_blank">arxiv</a>] 
    
    
    
  </span>
    </span>
    <span class="author">
      
      
       
       
       
          
            
              
                <a href="http://www.iliasdiakonikolas.org" target="_blank">  Ilias Diakonikolas</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="https://cseweb.ucsd.edu/~dakane" target="_blank">  Daniel M. Kane</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="http://vkonton.github.io" target="_blank">  Vasilis Kontonis</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="https://lteins.github.io" target="_blank">  Sihan Liu</a>, 
              
            
          
        
      
      
       
       
       
          
            
              and <em>Nikos Zarifis</em>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Advances in Neural Information Processing Systems (NeurIPS
  2023)</em>
    
 <!---   </span>
    <span class="notes"> -->
    
      
      </span>
  

<span id="DKKLZ23-materials">
 
    <!-- <li><a data-toggle="collapse" href="javascript:toggleDiv('DKKLZ23-bibtex')">BibTeX</a></li> -->
    <!-- <div id="DKKLZ23-bibtex" style="display:none;"> -->
    <!-- <pre class="collapse">@inproceedings{DKKLZ23,
  title = {Efficient Testable Learning of Halfspaces with Adversarial Label Noise},
  author = {Diakonikolas, Ilias and Kane, Daniel M. and Kontonis, Vasilis and Liu, Sihan and Zarifis, Nikos},
  year = {2023},
  arxiv = {2303.05485},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS
    2023)}
}
</pre> -->
    <!-- </div> -->


  
  <p id="DKKLZ23-abstract" class="collapse" style="border: 1px dashed
         black;padding:2px;
  font-size:11px">We give the first polynomial-time algorithm for the testable learning of
halfspaces in the presence of adversarial label noise under the Gaussian
distribution. In the recently introduced testable learning model, one is
required to produce a tester-learner such that if the data passes the tester,
then one can trust the output of the robust learner on the data. Our
tester-learner runs in time $\mathrm{poly}(d/\epsilon)$ and outputs a halfspace with
misclassification error $O(\mathrm{opt})+\epsilon$, where $\opt$ is the 0-1 error of the
best fitting halfspace. At a technical level, our algorithm employs an
iterative soft localization technique enhanced with appropriate testers to
ensure that the data distribution is sufficiently similar to a Gaussian.</p>
  

</span>
  <!-- <span class="links"> -->
  <!--  -->
  <!--   [<a class="abstract">Abstract</a>] -->
  <!--  -->
  <!--  -->
  <!--   [<a href="http://arxiv.org/abs/2303.05485"
    target="_blank">arXiv</a>] -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- </span> -->

  <!-- <!-1- Hidden abstract block -1-> -->
  <!--  -->
  <!-- <span class="abstract hidden"> -->
  <!--   <p>We give the first polynomial-time algorithm for the testable learning of
halfspaces in the presence of adversarial label noise under the Gaussian
distribution. In the recently introduced testable learning model, one is
required to produce a tester-learner such that if the data passes the tester,
then one can trust the output of the robust learner on the data. Our
tester-learner runs in time $\mathrm{poly}(d/\epsilon)$ and outputs a halfspace with
misclassification error $O(\mathrm{opt})+\epsilon$, where $\opt$ is the 0-1 error of the
best fitting halfspace. At a technical level, our algorithm employs an
iterative soft localization technique enhanced with appropriate testers to
ensure that the data distribution is sufficiently similar to a Gaussian.</p> -->
  <!-- </span> -->
  <!--  -->
</div>
</li>
<li>

<div id="DKTZ23">
  
    <span class="title">Self-Directed Linear Classification <span class="nav">
    
    [<a data-toggle="collapse" href="#DKTZ23-abstract">abstract</a>]
    
     
    [<a href="http://arxiv.org/abs/2308.03142" target="_blank">arxiv</a>] 
    
    
    
  </span>
    </span>
    <span class="author">
      
      
       
       
       
          
            
              
                <a href="http://www.iliasdiakonikolas.org" target="_blank">  Ilias Diakonikolas</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="http://vkonton.github.io" target="_blank">  Vasilis Kontonis</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="http://www.tzamos.com" target="_blank">  Christos Tzamos</a>, 
              
            
          
        
      
      
       
       
       
          
            
              and <em>Nikos Zarifis</em>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the 36th Annual Conference on Learning Theory
  (COLT 2023)</em>
    
 <!---   </span>
    <span class="notes"> -->
    
      
      </span>
  

<span id="DKTZ23-materials">
 
    <!-- <li><a data-toggle="collapse" href="javascript:toggleDiv('DKTZ23-bibtex')">BibTeX</a></li> -->
    <!-- <div id="DKTZ23-bibtex" style="display:none;"> -->
    <!-- <pre class="collapse">@inproceedings{DKTZ23,
  title = {Self-Directed Linear Classification},
  author = {Diakonikolas, Ilias and Kontonis, Vasilis and Tzamos, Christos and Zarifis, Nikos},
  year = {2023},
  arxiv = {2308.03142},
  booktitle = {Proceedings of the 36th Annual Conference on Learning Theory
    (COLT 2023)}
}
</pre> -->
    <!-- </div> -->


  
  <p id="DKTZ23-abstract" class="collapse" style="border: 1px dashed
         black;padding:2px;
  font-size:11px">In online classification, a learner is presented with a sequence of examples 
and aims to predict their labels in an online fashion so as to minimize 
the total number of mistakes. 
In the self-directed variant, the learner knows in advance 
the pool of examples and can adaptively choose the order in which predictions are made. 
Here we study the power of choosing the prediction order and establish 
the first strong separation between worst-order and random-order learning 
for the fundamental task of linear classification. 
Prior to our work, such a separation was known only for very restricted concept classes, 
e.g., one-dimensional thresholds or axis-aligned rectangles.
We present two main results.
If $X$ is a dataset of $n$ points drawn uniformly at random from the $d$-dimensional unit sphere, 
we design an efficient self-directed learner that
makes $O(d \log \log(n))$ mistakes and classifies the entire dataset.
If $X$ is an arbitrary $d$-dimensional dataset of size $n$, 
we design an efficient self-directed learner that predicts the labels 
of $99\%$ of the points in $X$ with mistake bound independent of $n$. 
In contrast, under a worst- or random-ordering, the number of mistakes 
must be at least $\Omega(d \log n)$, even when the 
points are drawn uniformly from the unit sphere 
and the learner only needs to predict the labels for $1\%$ of them.</p>
  

</span>
  <!-- <span class="links"> -->
  <!--  -->
  <!--   [<a class="abstract">Abstract</a>] -->
  <!--  -->
  <!--  -->
  <!--   [<a href="http://arxiv.org/abs/2308.03142"
    target="_blank">arXiv</a>] -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- </span> -->

  <!-- <!-1- Hidden abstract block -1-> -->
  <!--  -->
  <!-- <span class="abstract hidden"> -->
  <!--   <p>In online classification, a learner is presented with a sequence of examples 
and aims to predict their labels in an online fashion so as to minimize 
the total number of mistakes. 
In the self-directed variant, the learner knows in advance 
the pool of examples and can adaptively choose the order in which predictions are made. 
Here we study the power of choosing the prediction order and establish 
the first strong separation between worst-order and random-order learning 
for the fundamental task of linear classification. 
Prior to our work, such a separation was known only for very restricted concept classes, 
e.g., one-dimensional thresholds or axis-aligned rectangles.
We present two main results.
If $X$ is a dataset of $n$ points drawn uniformly at random from the $d$-dimensional unit sphere, 
we design an efficient self-directed learner that
makes $O(d \log \log(n))$ mistakes and classifies the entire dataset.
If $X$ is an arbitrary $d$-dimensional dataset of size $n$, 
we design an efficient self-directed learner that predicts the labels 
of $99\%$ of the points in $X$ with mistake bound independent of $n$. 
In contrast, under a worst- or random-ordering, the number of mistakes 
must be at least $\Omega(d \log n)$, even when the 
points are drawn uniformly from the unit sphere 
and the learner only needs to predict the labels for $1\%$ of them.</p> -->
  <!-- </span> -->
  <!--  -->
</div>
</li>
<li>

<div id="DDKWZ23">
  
    <span class="title">Information-Computation Tradeoffs for Learning Margin Halfspaces with Random Classification Noise <span class="nav">
    
    [<a data-toggle="collapse" href="#DDKWZ23-abstract">abstract</a>]
    
     
    [<a href="http://arxiv.org/abs/2306.16352" target="_blank">arxiv</a>] 
    
    
    
  </span>
    </span>
    <span class="author">
      
      
       
       
       
          
            
              
                <a href="http://www.iliasdiakonikolas.org" target="_blank">  Ilias Diakonikolas</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="https://www.jelena-diakonikolas.com" target="_blank">  Jelena Diakonikolas</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="https://cseweb.ucsd.edu/~dakane" target="_blank">  Daniel M. Kane</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="https://puqianwang.github.io" target="_blank">  Puqian Wang</a>, 
              
            
          
        
      
      
       
       
       
          
            
              and <em>Nikos Zarifis</em>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the 36th Annual Conference on Learning Theory
  (COLT 2023)</em>
    
 <!---   </span>
    <span class="notes"> -->
    
      
      </span>
  

<span id="DDKWZ23-materials">
 
    <!-- <li><a data-toggle="collapse" href="javascript:toggleDiv('DDKWZ23-bibtex')">BibTeX</a></li> -->
    <!-- <div id="DDKWZ23-bibtex" style="display:none;"> -->
    <!-- <pre class="collapse">@inproceedings{DDKWZ23,
  title = {Information-Computation Tradeoffs for Learning Margin Halfspaces with Random Classification Noise},
  author = {Diakonikolas, Ilias and Diakonikolas, Jelena and Kane, Daniel M. and Wang, Puqian and Zarifis, Nikos},
  year = {2023},
  arxiv = {2306.16352},
  booktitle = {Proceedings of the 36th Annual Conference on Learning Theory
    (COLT 2023)}
}
</pre> -->
    <!-- </div> -->


  
  <p id="DDKWZ23-abstract" class="collapse" style="border: 1px dashed
         black;padding:2px;
  font-size:11px">We study the problem of PAC learning $\gamma$-margin halfspaces 
with Random Classification Noise. 
We establish an information-computation tradeoff
suggesting an inherent gap between the sample complexity of the problem 
and the sample complexity of computationally efficient algorithms. 
Concretely, the sample complexity of the problem is $\widetilde{\Theta}(1/(\gamma^2 \epsilon))$. We start by giving a simple efficient algorithm 
with sample complexity $\widetilde{O}(1/(\gamma^2 \epsilon^2))$. Our main result
is a lower bound for Statistical Query (SQ) algorithms and low-degree polynomial tests suggesting that the quadratic dependence on $1/\epsilon$ 
in the sample complexity is inherent for computationally efficient algorithms.
Specifically, our results imply a lower bound of
$\widetilde{\Omega}(1/(\gamma^{1/2} \epsilon^2))$ on the sample complexity of
any efficient SQ learner or low-degree test.</p>
  

</span>
  <!-- <span class="links"> -->
  <!--  -->
  <!--   [<a class="abstract">Abstract</a>] -->
  <!--  -->
  <!--  -->
  <!--   [<a href="http://arxiv.org/abs/2306.16352"
    target="_blank">arXiv</a>] -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- </span> -->

  <!-- <!-1- Hidden abstract block -1-> -->
  <!--  -->
  <!-- <span class="abstract hidden"> -->
  <!--   <p>We study the problem of PAC learning $\gamma$-margin halfspaces 
with Random Classification Noise. 
We establish an information-computation tradeoff
suggesting an inherent gap between the sample complexity of the problem 
and the sample complexity of computationally efficient algorithms. 
Concretely, the sample complexity of the problem is $\widetilde{\Theta}(1/(\gamma^2 \epsilon))$. We start by giving a simple efficient algorithm 
with sample complexity $\widetilde{O}(1/(\gamma^2 \epsilon^2))$. Our main result
is a lower bound for Statistical Query (SQ) algorithms and low-degree polynomial tests suggesting that the quadratic dependence on $1/\epsilon$ 
in the sample complexity is inherent for computationally efficient algorithms.
Specifically, our results imply a lower bound of
$\widetilde{\Omega}(1/(\gamma^{1/2} \epsilon^2))$ on the sample complexity of
any efficient SQ learner or low-degree test.</p> -->
  <!-- </span> -->
  <!--  -->
</div>
</li>
<li>

<div id="DKPZ23">
  
    <span class="title">SQ Lower Bounds for Learning Mixtures of Separated and Bounded Covariance Gaussians <span class="nav">
    
    [<a data-toggle="collapse" href="#DKPZ23-abstract">abstract</a>]
    
     
    [<a href="http://arxiv.org/abs/2306.13057" target="_blank">arxiv</a>] 
    
    
    
  </span>
    </span>
    <span class="author">
      
      
       
       
       
          
            
              
                <a href="http://www.iliasdiakonikolas.org" target="_blank">  Ilias Diakonikolas</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="https://cseweb.ucsd.edu/~dakane" target="_blank">  Daniel M. Kane</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="https://thanasispittas.github.io" target="_blank">  Thanasis Pittas</a>, 
              
            
          
        
      
      
       
       
       
          
            
              and <em>Nikos Zarifis</em>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the 36th Annual Conference on Learning Theory
  (COLT 2023)</em>
    
 <!---   </span>
    <span class="notes"> -->
    
      
      </span>
  

<span id="DKPZ23-materials">
 
    <!-- <li><a data-toggle="collapse" href="javascript:toggleDiv('DKPZ23-bibtex')">BibTeX</a></li> -->
    <!-- <div id="DKPZ23-bibtex" style="display:none;"> -->
    <!-- <pre class="collapse">@inproceedings{DKPZ23,
  title = {SQ Lower Bounds for Learning Mixtures of Separated and Bounded Covariance Gaussians},
  author = {Diakonikolas, Ilias and Kane, Daniel M. and Pittas, Thanasis and Zarifis, Nikos},
  year = {2023},
  arxiv = {2306.13057},
  booktitle = {Proceedings of the 36th Annual Conference on Learning Theory
    (COLT 2023)},
  primaryclass = {cs.LG}
}
</pre> -->
    <!-- </div> -->


  
  <p id="DKPZ23-abstract" class="collapse" style="border: 1px dashed
         black;padding:2px;
  font-size:11px">We study the complexity of learning mixtures of separated Gaussians with common unknown bounded covariance matrix. 
Specifically, we focus on learning Gaussian mixture models (GMMs) 
on $\mathbb{R}^d$ of the form $P= \sum_{i=1}^k w_i \mathcal{N}(\vec \mu_i,\vec \Sigma_i)$, 
where $\vec \Sigma_i = \vec \Sigma \preceq \vec I$
and $\min_{i \neq j} \|\vec \mu_i - \vec \mu_j\|_2 \geq k^\epsilon$ for some $\epsilon&gt;0$. 
Known learning algorithms for this family of GMMs have complexity $(dk)^{O(1/\epsilon)}$. In this work, we prove that any Statistical Query (SQ) 
algorithm for this problem requires complexity at least $d^{\Omega(1/\epsilon)}$. 
Our SQ lower bound implies a similar lower bound for low-degree polynomial tests. 
Our result provides evidence that known algorithms for this problem are nearly
best possible.</p>
  

</span>
  <!-- <span class="links"> -->
  <!--  -->
  <!--   [<a class="abstract">Abstract</a>] -->
  <!--  -->
  <!--  -->
  <!--   [<a href="http://arxiv.org/abs/2306.13057"
    target="_blank">arXiv</a>] -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- </span> -->

  <!-- <!-1- Hidden abstract block -1-> -->
  <!--  -->
  <!-- <span class="abstract hidden"> -->
  <!--   <p>We study the complexity of learning mixtures of separated Gaussians with common unknown bounded covariance matrix. 
Specifically, we focus on learning Gaussian mixture models (GMMs) 
on $\mathbb{R}^d$ of the form $P= \sum_{i=1}^k w_i \mathcal{N}(\vec \mu_i,\vec \Sigma_i)$, 
where $\vec \Sigma_i = \vec \Sigma \preceq \vec I$
and $\min_{i \neq j} \|\vec \mu_i - \vec \mu_j\|_2 \geq k^\epsilon$ for some $\epsilon>0$. 
Known learning algorithms for this family of GMMs have complexity $(dk)^{O(1/\epsilon)}$. In this work, we prove that any Statistical Query (SQ) 
algorithm for this problem requires complexity at least $d^{\Omega(1/\epsilon)}$. 
Our SQ lower bound implies a similar lower bound for low-degree polynomial tests. 
Our result provides evidence that known algorithms for this problem are nearly
best possible.</p> -->
  <!-- </span> -->
  <!--  -->
</div>
</li>
<li>

<div id="WZDD23">
  
    <span class="title">Robustly Learning a Single Neuron via Sharpness <span class="nav">
    
    [<a data-toggle="collapse" href="#WZDD23-abstract">abstract</a>]
    
     
    [<a href="http://arxiv.org/abs/2306.07892" target="_blank">arxiv</a>] 
    
    
    
  </span>
    </span>
    <span class="author">
      
      
       
       
       
          
            
              
              Puqian  Wang*,
              
            
          
        
      
      
       
       
       
          
            
              <em>Nikos Zarifis*</em>,
            
          
        
      
      
       
       
       
          
            
              
                <a href="http://www.iliasdiakonikolas.org" target="_blank">  Ilias Diakonikolas</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                and <a href="https://www.jelena-diakonikolas.com" target="_blank">Jelena Diakonikolas</a> 
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the 40th International Conference on Machine
  Learning (ICML 2023)</em>
    
 <!---   </span>
    <span class="notes"> -->
    
      <b> [Selected for Oral Presentation]</b>
        
      
      <small>  * Equal contribution</small>
        
      </span>
  

<span id="WZDD23-materials">
 
    <!-- <li><a data-toggle="collapse" href="javascript:toggleDiv('WZDD23-bibtex')">BibTeX</a></li> -->
    <!-- <div id="WZDD23-bibtex" style="display:none;"> -->
    <!-- <pre class="collapse">@inproceedings{WZDD23,
  title = {Robustly Learning a Single Neuron via Sharpness},
  author = {Wang*, Puqian and Zarifis*, Nikos and Diakonikolas, Ilias and Diakonikolas, Jelena},
  year = {2023},
  booktitle = {Proceedings of the 40th International Conference on Machine
    Learning (ICML 2023)},
  arxiv = {2306.07892},
  notes = {* Equal contribution},
  bnotes = {[Selected for Oral Presentation]}
}
</pre> -->
    <!-- </div> -->


  
  <p id="WZDD23-abstract" class="collapse" style="border: 1px dashed
         black;padding:2px;
  font-size:11px">We study the problem of learning a single neuron with respect to the 
-loss in the presence of adversarial label noise. We give an efficient algorithm that, for a broad family of activations including ReLUs, approximates the optimal 
$L_2^2$-error within a constant factor. Notably, our algorithm applies under much milder distributional assumptions compared to prior work. The key ingredient enabling our results is a novel connection to local error bounds from optimization theory.</p>
  

</span>
  <!-- <span class="links"> -->
  <!--  -->
  <!--   [<a class="abstract">Abstract</a>] -->
  <!--  -->
  <!--  -->
  <!--   [<a href="http://arxiv.org/abs/2306.07892"
    target="_blank">arXiv</a>] -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- </span> -->

  <!-- <!-1- Hidden abstract block -1-> -->
  <!--  -->
  <!-- <span class="abstract hidden"> -->
  <!--   <p>We study the problem of learning a single neuron with respect to the 
-loss in the presence of adversarial label noise. We give an efficient algorithm that, for a broad family of activations including ReLUs, approximates the optimal 
$L_2^2$-error within a constant factor. Notably, our algorithm applies under much milder distributional assumptions compared to prior work. The key ingredient enabling our results is a novel connection to local error bounds from optimization theory.</p> -->
  <!-- </span> -->
  <!--  -->
</div>
</li>
<li>

<div id="DKTZ22a">
  
    <span class="title">Learning a Single Neuron with Adversarial Label Noise via Gradient Descent <span class="nav">
    
    [<a data-toggle="collapse" href="#DKTZ22a-abstract">abstract</a>]
    
     
    [<a href="http://arxiv.org/abs/2206.08918" target="_blank">arxiv</a>] 
    
    
    
  </span>
    </span>
    <span class="author">
      
      
       
       
       
          
            
              
                <a href="http://www.iliasdiakonikolas.org" target="_blank">  Ilias Diakonikolas</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="http://vkonton.github.io" target="_blank">  Vasilis Kontonis</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="http://www.tzamos.com" target="_blank">  Christos Tzamos</a>, 
              
            
          
        
      
      
       
       
       
          
            
              and <em>Nikos Zarifis</em>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the 35th Annual Conference on Learning Theory (COLT 2022)</em>
    
 <!---   </span>
    <span class="notes"> -->
    
      
      </span>
  

<span id="DKTZ22a-materials">
 
    <!-- <li><a data-toggle="collapse" href="javascript:toggleDiv('DKTZ22a-bibtex')">BibTeX</a></li> -->
    <!-- <div id="DKTZ22a-bibtex" style="display:none;"> -->
    <!-- <pre class="collapse">@inproceedings{DKTZ22a,
  title = {Learning a Single Neuron with Adversarial Label Noise via Gradient Descent},
  author = {Diakonikolas, Ilias and Kontonis, Vasilis and Tzamos, Christos and Zarifis, Nikos},
  year = {2022},
  arxiv = {2206.08918},
  booktitle = {Proceedings of the 35th Annual Conference on Learning Theory (COLT 2022)}
}
</pre> -->
    <!-- </div> -->


  
  <p id="DKTZ22a-abstract" class="collapse" style="border: 1px dashed
         black;padding:2px;
  font-size:11px">We study the fundamental problem of learning a single neuron, i.e., a 
function of the form $\mathbf{x} \mapsto \sigma(\mathbf{ w} \cdot \mathbf{x})$ for monotone activations 
$\sigma:\mathbb{R} \mapsto \mathbb{R}$, with respect to the $L_2^2$-loss in the presence of adversarial label noise. 
Specifically, we are given labeled examples from a distribution $D$ on
$(\mathbf{x}, y) \in \mathbb{R}^d \times \mathbb{R}$
such that there exists $\mathbf{w}^\ast \in \R^d$ achieving $F(\mathbf{ w}^\ast)
\leq \eps$, where
$F(\mathbf{ w}) = \mathbb{E}_{(\mathbf{x},y) \sim D}[(\sigma(\mathbf{ w}\cdot
\mathbf{x}) - y)^2]$. The goal of the learner
is to output a hypothesis vector $\wt{\vec w}$ such that $F(\wt{\vec w}) = C \,\eps$ with 
high probability, where $C$ is a universal constant. As our main contribution, we give
efficient constant-factor approximate learners 
for a broad class of distributions (including log-concave distributions)
and activation functions (including ReLUs and sigmoids).
Concretely, for the class of isotropic log-concave distributions, we obtain
the following important corollaries:
1) For the logistic activation, i.e., $\sigma(t) = 1/(1+e^{-t})$, we obtain the first
polynomial-time constant factor approximation, even under the Gaussian distribution.
Moreover, our algorithm has sample complexity $\wt{O}(d/\eps)$, which is tight within 
polylogarithmic factors. 

2) For the ReLU activation, i.e., $\sigma(t) = \max(0,t)$, we give an efficient algorithm with 
sample complexity $\wt{O}(d \, \polylog(1/\eps))$. Prior to our work, the best known 
constant-factor approximate learner had sample complexity $\Omega(d/\eps)$.
In both settings, our algorithms are simple, performing gradient-descent on the (regularized) $L_2^2$-loss. 
The correctness of our algorithms relies on novel structural results that we establish, 
showing that (essentially all) stationary points of the underlying non-convex loss
are approximately optimal.</p>
  

</span>
  <!-- <span class="links"> -->
  <!--  -->
  <!--   [<a class="abstract">Abstract</a>] -->
  <!--  -->
  <!--  -->
  <!--   [<a href="http://arxiv.org/abs/2206.08918"
    target="_blank">arXiv</a>] -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- </span> -->

  <!-- <!-1- Hidden abstract block -1-> -->
  <!--  -->
  <!-- <span class="abstract hidden"> -->
  <!--   <p>We study the fundamental problem of learning a single neuron, i.e., a 
function of the form $\mathbf{x} \mapsto \sigma(\mathbf{ w} \cdot \mathbf{x})$ for monotone activations 
$\sigma:\mathbb{R} \mapsto \mathbb{R}$, with respect to the $L_2^2$-loss in the presence of adversarial label noise. 
Specifically, we are given labeled examples from a distribution $D$ on
$(\mathbf{x}, y) \in \mathbb{R}^d \times \mathbb{R}$
such that there exists $\mathbf{w}^\ast \in \R^d$ achieving $F(\mathbf{ w}^\ast)
\leq \eps$, where
$F(\mathbf{ w}) = \mathbb{E}_{(\mathbf{x},y) \sim D}[(\sigma(\mathbf{ w}\cdot
\mathbf{x}) - y)^2]$. The goal of the learner
is to output a hypothesis vector $\wt{\vec w}$ such that $F(\wt{\vec w}) = C \,\eps$ with 
high probability, where $C$ is a universal constant. As our main contribution, we give
efficient constant-factor approximate learners 
for a broad class of distributions (including log-concave distributions)
and activation functions (including ReLUs and sigmoids).
Concretely, for the class of isotropic log-concave distributions, we obtain
the following important corollaries:
1) For the logistic activation, i.e., $\sigma(t) = 1/(1+e^{-t})$, we obtain the first
polynomial-time constant factor approximation, even under the Gaussian distribution.
Moreover, our algorithm has sample complexity $\wt{O}(d/\eps)$, which is tight within 
polylogarithmic factors. 

2) For the ReLU activation, i.e., $\sigma(t) = \max(0,t)$, we give an efficient algorithm with 
sample complexity $\wt{O}(d \, \polylog(1/\eps))$. Prior to our work, the best known 
constant-factor approximate learner had sample complexity $\Omega(d/\eps)$.
In both settings, our algorithms are simple, performing gradient-descent on the (regularized) $L_2^2$-loss. 
The correctness of our algorithms relies on novel structural results that we establish, 
showing that (essentially all) stationary points of the underlying non-convex loss
are approximately optimal.</p> -->
  <!-- </span> -->
  <!--  -->
</div>
</li>
<li>

<div id="DKTZ22b">
  
    <span class="title">Learning General Halfspaces with Adversarial Label Noise via Online Gradient Descent <span class="nav">
    
    [<a data-toggle="collapse" href="#DKTZ22b-abstract">abstract</a>]
    
     
    
    
  </span>
    </span>
    <span class="author">
      
      
       
       
       
          
            
              
                <a href="http://www.iliasdiakonikolas.org" target="_blank">  Ilias Diakonikolas</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="http://vkonton.github.io" target="_blank">  Vasilis Kontonis</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="http://www.tzamos.com" target="_blank">  Christos Tzamos</a>, 
              
            
          
        
      
      
       
       
       
          
            
              and <em>Nikos Zarifis</em>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the 39th International Conference on Machine Learning (ICML 2022)</em>
    
 <!---   </span>
    <span class="notes"> -->
    
      
      </span>
  

<span id="DKTZ22b-materials">
 
    <!-- <li><a data-toggle="collapse" href="javascript:toggleDiv('DKTZ22b-bibtex')">BibTeX</a></li> -->
    <!-- <div id="DKTZ22b-bibtex" style="display:none;"> -->
    <!-- <pre class="collapse">@inproceedings{DKTZ22b,
  title = {Learning General Halfspaces with Adversarial Label Noise via Online Gradient Descent},
  author = {Diakonikolas, Ilias and Kontonis, Vasilis and Tzamos, Christos and Zarifis, Nikos},
  year = {2022},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning (ICML 2022)}
}
</pre> -->
    <!-- </div> -->


  
  <p id="DKTZ22b-abstract" class="collapse" style="border: 1px dashed
         black;padding:2px;
  font-size:11px">We study the problem of learning general — i.e., not necessarily homogeneous — halfspaces with adversarial label noise under the Gaussian distribution. Prior work has provided a sophisticated polynomial-time algorithm for this problem. In this work, we show that the problem can be solved directly via online gradient descent applied to a sequence of natural non-convex surrogates. This approach yields a simple iterative learning algorithm for general halfspaces with near-optimal sample complexity, runtime, and error guarantee. At the conceptual level, our work establishes an intriguing connection between learning halfspaces with adversarial noise and online optimization that may find other applications.</p>
  

</span>
  <!-- <span class="links"> -->
  <!--  -->
  <!--   [<a class="abstract">Abstract</a>] -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- </span> -->

  <!-- <!-1- Hidden abstract block -1-> -->
  <!--  -->
  <!-- <span class="abstract hidden"> -->
  <!--   <p>We study the problem of learning general — i.e., not necessarily homogeneous — halfspaces with adversarial label noise under the Gaussian distribution. Prior work has provided a sophisticated polynomial-time algorithm for this problem. In this work, we show that the problem can be solved directly via online gradient descent applied to a sequence of natural non-convex surrogates. This approach yields a simple iterative learning algorithm for general halfspaces with near-optimal sample complexity, runtime, and error guarantee. At the conceptual level, our work establishes an intriguing connection between learning halfspaces with adversarial noise and online optimization that may find other applications.</p> -->
  <!-- </span> -->
  <!--  -->
</div>
</li>
<li>

<div id="DKKTZ21b">
  
    <span class="title">Learning General Halfspaces with General Massart Noise under the
 Gaussian Distribution <span class="nav">
    
    [<a data-toggle="collapse" href="#DKKTZ21b-abstract">abstract</a>]
    
     
    [<a href="http://arxiv.org/abs/2108.08767" target="_blank">arxiv</a>] 
    
    
    
  </span>
    </span>
    <span class="author">
      
      
       
       
       
          
            
              
                <a href="http://www.iliasdiakonikolas.org" target="_blank">  Ilias Diakonikolas</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="https://cseweb.ucsd.edu/~dakane" target="_blank">  Daniel M. Kane</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="http://vkonton.github.io" target="_blank">  Vasilis Kontonis</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="http://www.tzamos.com" target="_blank">  Christos Tzamos</a>, 
              
            
          
        
      
      
       
       
       
          
            
              and <em>Nikos Zarifis</em>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the 54th Annual ACM Symposium on Theory of Computing (STOC 2022)</em>
    
 <!---   </span>
    <span class="notes"> -->
    
      
      </span>
  

<span id="DKKTZ21b-materials">
 
    <!-- <li><a data-toggle="collapse" href="javascript:toggleDiv('DKKTZ21b-bibtex')">BibTeX</a></li> -->
    <!-- <div id="DKKTZ21b-bibtex" style="display:none;"> -->
    <!-- <pre class="collapse">@inproceedings{DKKTZ21b,
  title = {Learning General Halfspaces with General Massart Noise under the
   Gaussian Distribution},
  author = {Diakonikolas, Ilias and Kane, Daniel M. and Kontonis, Vasilis and Tzamos, Christos and Zarifis, Nikos},
  year = {2022},
  booktitle = {Proceedings of the 54th Annual ACM Symposium on Theory of Computing (STOC 2022)},
  arxiv = {2108.08767},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG}
}
</pre> -->
    <!-- </div> -->


  
  <p id="DKKTZ21b-abstract" class="collapse" style="border: 1px dashed
         black;padding:2px;
  font-size:11px">We study the problem of PAC learning halfspaces on $\mathbb{R}^d$ with
Massart noise under the Gaussian distribution. In the Massart model, an
adversary is allowed to flip the label of each point $\mathbf{x}$ with unknown
probability $\eta(\mathbf{x}) \leq \eta$, for some parameter $\eta \in
[0,1/2]$. The goal is to find a hypothesis with misclassification error of
$\mathrm{OPT} + \epsilon$, where $\mathrm{OPT}$ is the error of the target
halfspace. This problem had been previously studied under two assumptions: (i)
the target halfspace is homogeneous (i.e., the separating hyperplane goes
through the origin), and (ii) the parameter $\eta$ is strictly smaller than
$1/2$. Prior to this work, no nontrivial bounds were known when either of these
assumptions is removed. We study the general problem and establish the
following:
 For $\eta &lt;1/2$, we give a learning algorithm for general halfspaces with
sample and computational complexity
$d^{O_{\eta}(\log(1/\gamma))}\mathrm{poly}(1/\epsilon)$, where $\gamma
=\max\{\epsilon, \min\{\mathbf{Pr}[f(\mathbf{x}) = 1],
\mathbf{Pr}[f(\mathbf{x}) = -1]\} \}$ is the bias of the target halfspace $f$.
Prior efficient algorithms could only handle the special case of $\gamma =
1/2$. Interestingly, we establish a qualitatively matching lower bound of
$d^{\Omega(\log(1/\gamma))}$ on the complexity of any Statistical Query (SQ)
algorithm.
 For $\eta = 1/2$, we give a learning algorithm for general halfspaces with
sample and computational complexity $O_\epsilon(1) d^{O(\log(1/\epsilon))}$.
This result is new even for the subclass of homogeneous halfspaces; prior
algorithms for homogeneous Massart halfspaces provide vacuous guarantees for
$\eta=1/2$. We complement our upper bound with a nearly-matching SQ lower bound
of $d^{\Omega(\log(1/\epsilon))}$, which holds even for the special case of
homogeneous halfspaces.</p>
  

</span>
  <!-- <span class="links"> -->
  <!--  -->
  <!--   [<a class="abstract">Abstract</a>] -->
  <!--  -->
  <!--  -->
  <!--   [<a href="http://arxiv.org/abs/2108.08767"
    target="_blank">arXiv</a>] -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- </span> -->

  <!-- <!-1- Hidden abstract block -1-> -->
  <!--  -->
  <!-- <span class="abstract hidden"> -->
  <!--   <p>We study the problem of PAC learning halfspaces on $\mathbb{R}^d$ with
Massart noise under the Gaussian distribution. In the Massart model, an
adversary is allowed to flip the label of each point $\mathbf{x}$ with unknown
probability $\eta(\mathbf{x}) \leq \eta$, for some parameter $\eta \in
[0,1/2]$. The goal is to find a hypothesis with misclassification error of
$\mathrm{OPT} + \epsilon$, where $\mathrm{OPT}$ is the error of the target
halfspace. This problem had been previously studied under two assumptions: (i)
the target halfspace is homogeneous (i.e., the separating hyperplane goes
through the origin), and (ii) the parameter $\eta$ is strictly smaller than
$1/2$. Prior to this work, no nontrivial bounds were known when either of these
assumptions is removed. We study the general problem and establish the
following:
 For $\eta <1/2$, we give a learning algorithm for general halfspaces with
sample and computational complexity
$d^{O_{\eta}(\log(1/\gamma))}\mathrm{poly}(1/\epsilon)$, where $\gamma
=\max\{\epsilon, \min\{\mathbf{Pr}[f(\mathbf{x}) = 1],
\mathbf{Pr}[f(\mathbf{x}) = -1]\} \}$ is the bias of the target halfspace $f$.
Prior efficient algorithms could only handle the special case of $\gamma =
1/2$. Interestingly, we establish a qualitatively matching lower bound of
$d^{\Omega(\log(1/\gamma))}$ on the complexity of any Statistical Query (SQ)
algorithm.
 For $\eta = 1/2$, we give a learning algorithm for general halfspaces with
sample and computational complexity $O_\epsilon(1) d^{O(\log(1/\epsilon))}$.
This result is new even for the subclass of homogeneous halfspaces; prior
algorithms for homogeneous Massart halfspaces provide vacuous guarantees for
$\eta=1/2$. We complement our upper bound with a nearly-matching SQ lower bound
of $d^{\Omega(\log(1/\epsilon))}$, which holds even for the special case of
homogeneous halfspaces.</p> -->
  <!-- </span> -->
  <!--  -->
</div>
</li>
<li>

<div id="DKKTZ21">
  
    <span class="title">Agnostic Proper Learning of Halfspaces under Gaussian Marginals <span class="nav">
    
    [<a data-toggle="collapse" href="#DKKTZ21-abstract">abstract</a>]
    
     
    [<a href="http://arxiv.org/abs/2102.05629" target="_blank">arxiv</a>] 
    
    
    
  </span>
    </span>
    <span class="author">
      
      
       
       
       
          
            
              
                <a href="http://www.iliasdiakonikolas.org" target="_blank">  Ilias Diakonikolas</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="https://cseweb.ucsd.edu/~dakane" target="_blank">  Daniel M. Kane</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="http://vkonton.github.io" target="_blank">  Vasilis Kontonis</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="http://www.tzamos.com" target="_blank">  Christos Tzamos</a>, 
              
            
          
        
      
      
       
       
       
          
            
              and <em>Nikos Zarifis</em>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the 34th Annual Conference on Learning Theory (COLT 2021)</em>
    
 <!---   </span>
    <span class="notes"> -->
    
      
      </span>
  

<span id="DKKTZ21-materials">
 
    <!-- <li><a data-toggle="collapse" href="javascript:toggleDiv('DKKTZ21-bibtex')">BibTeX</a></li> -->
    <!-- <div id="DKKTZ21-bibtex" style="display:none;"> -->
    <!-- <pre class="collapse">@inproceedings{DKKTZ21,
  title = {Agnostic Proper Learning of Halfspaces under Gaussian Marginals},
  author = {Diakonikolas, Ilias and Kane, Daniel M. and Kontonis, Vasilis and Tzamos, Christos and Zarifis, Nikos},
  year = {2021},
  booktitle = {Proceedings of the 34th Annual Conference on Learning Theory (COLT 2021)},
  arxiv = {2102.05629},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG}
}
</pre> -->
    <!-- </div> -->


  
  <p id="DKKTZ21-abstract" class="collapse" style="border: 1px dashed
         black;padding:2px;
  font-size:11px">We study the problem of agnostically learning halfspaces under the Gaussian distribution. Our main result is the {\em first proper} learning algorithm for this problem whose sample complexity and computational complexity qualitatively match those of the best known improper agnostic learner. Building on this result, we also obtain the first proper polynomial-time approximation scheme (PTAS) for agnostically learning homogeneous halfspaces. Our techniques naturally extend to agnostically learning linear models with respect to other non-linear activations, yielding in particular the first proper agnostic algorithm for ReLU regression.</p>
  

</span>
  <!-- <span class="links"> -->
  <!--  -->
  <!--   [<a class="abstract">Abstract</a>] -->
  <!--  -->
  <!--  -->
  <!--   [<a href="http://arxiv.org/abs/2102.05629"
    target="_blank">arXiv</a>] -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- </span> -->

  <!-- <!-1- Hidden abstract block -1-> -->
  <!--  -->
  <!-- <span class="abstract hidden"> -->
  <!--   <p>We study the problem of agnostically learning halfspaces under the Gaussian distribution. Our main result is the {\em first proper} learning algorithm for this problem whose sample complexity and computational complexity qualitatively match those of the best known improper agnostic learner. Building on this result, we also obtain the first proper polynomial-time approximation scheme (PTAS) for agnostically learning homogeneous halfspaces. Our techniques naturally extend to agnostically learning linear models with respect to other non-linear activations, yielding in particular the first proper agnostic algorithm for ReLU regression.</p> -->
  <!-- </span> -->
  <!--  -->
</div>
</li>
<li>

<div id="DKPZ21">
  
    <span class="title">The Optimality of Polynomial Regression for Agnostic Learning under Gaussian Marginals <span class="nav">
    
    [<a data-toggle="collapse" href="#DKPZ21-abstract">abstract</a>]
    
     
    [<a href="http://arxiv.org/abs/2102.04401" target="_blank">arxiv</a>] 
    
    
    
  </span>
    </span>
    <span class="author">
      
      
       
       
       
          
            
              
                <a href="http://www.iliasdiakonikolas.org" target="_blank">  Ilias Diakonikolas</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="https://cseweb.ucsd.edu/~dakane" target="_blank">  Daniel M. Kane</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="https://thanasispittas.github.io" target="_blank">  Thanasis Pittas</a>, 
              
            
          
        
      
      
       
       
       
          
            
              and <em>Nikos Zarifis</em>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the 34th Annual Conference on Learning Theory (COLT 2021)</em>
    
 <!---   </span>
    <span class="notes"> -->
    
      
      </span>
  

<span id="DKPZ21-materials">
 
    <!-- <li><a data-toggle="collapse" href="javascript:toggleDiv('DKPZ21-bibtex')">BibTeX</a></li> -->
    <!-- <div id="DKPZ21-bibtex" style="display:none;"> -->
    <!-- <pre class="collapse">@inproceedings{DKPZ21,
  title = {The Optimality of Polynomial Regression for Agnostic Learning under Gaussian Marginals},
  author = {Diakonikolas, Ilias and Kane, Daniel M. and Pittas, Thanasis and Zarifis, Nikos},
  year = {2021},
  booktitle = {Proceedings of the 34th Annual Conference on Learning Theory (COLT 2021)},
  arxiv = {2102.04401},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG}
}
</pre> -->
    <!-- </div> -->


  
  <p id="DKPZ21-abstract" class="collapse" style="border: 1px dashed
         black;padding:2px;
  font-size:11px">We study the problem of agnostic learning under the Gaussian distribution. We develop a method for finding hard families of examples for a wide class of problems by using LP duality. For Boolean-valued concept classes, we show that the $L^1$-regression algorithm is essentially best possible, and therefore that the computational difficulty of agnostically learning a concept class is closely related to the polynomial degree required to approximate any function from the class in $L^1$-norm. Using this characterization along with additional analytic tools, we obtain optimal SQ lower bounds for agnostically learning linear threshold functions and the first non-trivial SQ lower bounds for polynomial threshold functions and intersections of halfspaces. We also develop an analogous theory for agnostically learning real-valued functions, and as an application prove near-optimal SQ lower bounds for agnostically learning ReLUs and sigmoids.</p>
  

</span>
  <!-- <span class="links"> -->
  <!--  -->
  <!--   [<a class="abstract">Abstract</a>] -->
  <!--  -->
  <!--  -->
  <!--   [<a href="http://arxiv.org/abs/2102.04401"
    target="_blank">arXiv</a>] -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- </span> -->

  <!-- <!-1- Hidden abstract block -1-> -->
  <!--  -->
  <!-- <span class="abstract hidden"> -->
  <!--   <p>We study the problem of agnostic learning under the Gaussian distribution. We develop a method for finding hard families of examples for a wide class of problems by using LP duality. For Boolean-valued concept classes, we show that the $L^1$-regression algorithm is essentially best possible, and therefore that the computational difficulty of agnostically learning a concept class is closely related to the polynomial degree required to approximate any function from the class in $L^1$-norm. Using this characterization along with additional analytic tools, we obtain optimal SQ lower bounds for agnostically learning linear threshold functions and the first non-trivial SQ lower bounds for polynomial threshold functions and intersections of halfspaces. We also develop an analogous theory for agnostically learning real-valued functions, and as an application prove near-optimal SQ lower bounds for agnostically learning ReLUs and sigmoids.</p> -->
  <!-- </span> -->
  <!--  -->
</div>
</li>
<li>

<div id="DKTVZ21">
  
    <span class="title">Learning Online Algorithms with Distributional Advice <span class="nav">
    
     
    
    
  </span>
    </span>
    <span class="author">
      
      
       
       
       
          
            
              
                <a href="http://www.iliasdiakonikolas.org" target="_blank">  Ilias Diakonikolas</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="http://vkonton.github.io" target="_blank">  Vasilis Kontonis</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="http://www.tzamos.com" target="_blank">  Christos Tzamos</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="http://www.mit.edu/~vakilian/" target="_blank">  Ali Vakilian</a>, 
              
            
          
        
      
      
       
       
       
          
            
              and <em>Nikos Zarifis</em>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the 38th International Conference on Machine Learning (ICML 2021)</em>
    
 <!---   </span>
    <span class="notes"> -->
    
      
      </span>
  

<span id="DKTVZ21-materials">
 
    <!-- <li><a data-toggle="collapse" href="javascript:toggleDiv('DKTVZ21-bibtex')">BibTeX</a></li> -->
    <!-- <div id="DKTVZ21-bibtex" style="display:none;"> -->
    <!-- <pre class="collapse">@inproceedings{DKTVZ21,
  title = {Learning Online Algorithms with Distributional Advice},
  author = {Diakonikolas, Ilias and Kontonis, Vasilis and Tzamos, Christos and Vakilian, Ali and Zarifis, Nikos},
  year = {2020},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning (ICML 2021)},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG}
}
</pre> -->
    <!-- </div> -->


  

</span>
  <!-- <span class="links"> -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- </span> -->

  <!-- <!-1- Hidden abstract block -1-> -->
  <!--  -->
</div>
</li>
<li>

<div id="DKKTZ20">
  
    <span class="title">A Polynomial Time Algorithm for Learning Halfspaces with Tsybakov Noise <span class="nav">
    
    [<a data-toggle="collapse" href="#DKKTZ20-abstract">abstract</a>]
    
     
    [<a href="http://arxiv.org/abs/2010.01705" target="_blank">arxiv</a>] 
    
    
    
  </span>
    </span>
    <span class="author">
      
      
       
       
       
          
            
              
                <a href="http://www.iliasdiakonikolas.org" target="_blank">  Ilias Diakonikolas</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="https://cseweb.ucsd.edu/~dakane" target="_blank">  Daniel M. Kane</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="http://vkonton.github.io" target="_blank">  Vasilis Kontonis</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="http://www.tzamos.com" target="_blank">  Christos Tzamos</a>, 
              
            
          
        
      
      
       
       
       
          
            
              and <em>Nikos Zarifis</em>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the 53rd Annual ACM Symposium on Theory of Computing (STOC 2021)</em>
    
 <!---   </span>
    <span class="notes"> -->
    
      
      </span>
  

<span id="DKKTZ20-materials">
 
    <!-- <li><a data-toggle="collapse" href="javascript:toggleDiv('DKKTZ20-bibtex')">BibTeX</a></li> -->
    <!-- <div id="DKKTZ20-bibtex" style="display:none;"> -->
    <!-- <pre class="collapse">@inproceedings{DKKTZ20,
  title = {A Polynomial Time Algorithm for Learning Halfspaces with Tsybakov Noise},
  author = {Diakonikolas, Ilias and Kane, Daniel M. and Kontonis, Vasilis and Tzamos, Christos and Zarifis, Nikos},
  year = {2020},
  booktitle = {Proceedings of the 53rd Annual ACM Symposium on Theory of Computing (STOC 2021)},
  arxiv = {2010.01705},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG}
}
</pre> -->
    <!-- </div> -->


  
  <p id="DKKTZ20-abstract" class="collapse" style="border: 1px dashed
         black;padding:2px;
  font-size:11px">We study the problem of PAC learning homogeneous halfspaces in the presence of Tsybakov noise. In the Tsybakov noise model, the label of every sample is independently flipped with an adversarially controlled probability that can be arbitrarily close to $1/2$ for a fraction of the samples. {\em We give the first polynomial-time algorithm for this fundamental learning problem.} Our algorithm learns the true halfspace within any desired accuracy $\epsilon$ and succeeds under a broad family of well-behaved distributions including log-concave distributions. Prior to our work, the only previous algorithm for this problem required quasi-polynomial runtime in $1/\epsilon$.
Our algorithm employs a recently developed reduction DKTZ20b from learning to
certifying the non-optimality of a candidate halfspace. This prior work
developed a quasi-polynomial time certificate algorithm based on polynomial
regression. {\em The main technical contribution of the current paper is the
first polynomial-time certificate algorithm.} Starting from a non-trivial
warm-start, our algorithm performs a novel "win-win" iterative process which, at
each step, either finds a valid certificate or improves the angle between the
current halfspace and the true one. Our warm-start algorithm for isotropic
log-concave distributions involves a number of analytic tools that may be of
broader interest. These include a new efficient method for reweighting the
distribution in order to recenter it and a novel characterization of the
spectrum of the degree-$2$ Chow parameters.</p>
  

</span>
  <!-- <span class="links"> -->
  <!--  -->
  <!--   [<a class="abstract">Abstract</a>] -->
  <!--  -->
  <!--  -->
  <!--   [<a href="http://arxiv.org/abs/2010.01705"
    target="_blank">arXiv</a>] -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- </span> -->

  <!-- <!-1- Hidden abstract block -1-> -->
  <!--  -->
  <!-- <span class="abstract hidden"> -->
  <!--   <p>We study the problem of PAC learning homogeneous halfspaces in the presence of Tsybakov noise. In the Tsybakov noise model, the label of every sample is independently flipped with an adversarially controlled probability that can be arbitrarily close to $1/2$ for a fraction of the samples. {\em We give the first polynomial-time algorithm for this fundamental learning problem.} Our algorithm learns the true halfspace within any desired accuracy $\epsilon$ and succeeds under a broad family of well-behaved distributions including log-concave distributions. Prior to our work, the only previous algorithm for this problem required quasi-polynomial runtime in $1/\epsilon$.
Our algorithm employs a recently developed reduction DKTZ20b from learning to
certifying the non-optimality of a candidate halfspace. This prior work
developed a quasi-polynomial time certificate algorithm based on polynomial
regression. {\em The main technical contribution of the current paper is the
first polynomial-time certificate algorithm.} Starting from a non-trivial
warm-start, our algorithm performs a novel "win-win" iterative process which, at
each step, either finds a valid certificate or improves the angle between the
current halfspace and the true one. Our warm-start algorithm for isotropic
log-concave distributions involves a number of analytic tools that may be of
broader interest. These include a new efficient method for reweighting the
distribution in order to recenter it and a novel characterization of the
spectrum of the degree-$2$ Chow parameters.</p> -->
  <!-- </span> -->
  <!--  -->
</div>
</li>
<li>

<div id="DKTZ20b">
  
    <span class="title">Learning Halfspaces with Tsybakov Noise <span class="nav">
    
    [<a data-toggle="collapse" href="#DKTZ20b-abstract">abstract</a>]
    
     
    [<a href="http://arxiv.org/abs/2006.06467" target="_blank">arxiv</a>] 
    
    
    
  </span>
    </span>
    <span class="author">
      
      
       
       
       
          
            
              
                <a href="http://www.iliasdiakonikolas.org" target="_blank">  Ilias Diakonikolas</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="http://vkonton.github.io" target="_blank">  Vasilis Kontonis</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="http://www.tzamos.com" target="_blank">  Christos Tzamos</a>, 
              
            
          
        
      
      
       
       
       
          
            
              and <em>Nikos Zarifis</em>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the 53rd Annual ACM Symposium on Theory of Computing (STOC 2021)
(Conference version to be merged with paper above.)</em>
    
 <!---   </span>
    <span class="notes"> -->
    
      
      </span>
  

<span id="DKTZ20b-materials">
 
    <!-- <li><a data-toggle="collapse" href="javascript:toggleDiv('DKTZ20b-bibtex')">BibTeX</a></li> -->
    <!-- <div id="DKTZ20b-bibtex" style="display:none;"> -->
    <!-- <pre class="collapse">@inproceedings{DKTZ20b,
  title = {Learning Halfspaces with Tsybakov Noise},
  author = {Diakonikolas, Ilias and Kontonis, Vasilis and Tzamos, Christos and Zarifis, Nikos},
  year = {2020},
  eprint = {2006.06467},
  arxiv = {2006.06467},
  booktitle = {Proceedings of the 53rd Annual ACM Symposium on Theory of Computing (STOC 2021)
  (Conference version to be merged with paper above.)},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG}
}
</pre> -->
    <!-- </div> -->


  
  <p id="DKTZ20b-abstract" class="collapse" style="border: 1px dashed
         black;padding:2px;
  font-size:11px">We study the efficient PAC
  learnability of halfspaces in the presence of Tsybakov noise. In the Tsybakov
  noise model, each label is independently flipped with some probability which
  is controlled by an adversary. This noise model significantly generalizes the
  Massart noise model, by allowing the flipping probabilities to be arbitrarily
  close to $1/2$ for a fraction of the samples. Our main result is the first
  non-trivial PAC learning algorithm for this problem under a broad family of
  structured distributions -- satisfying certain concentration and
  (anti-)anti-concentration properties -- including log-concave distributions.
  Specifically, we given an algorithm that achieves misclassification error
  $\epsilon$ with respect to the true halfspace, with quasi-polynomial runtime
  dependence in $1/\epsilon$. The only previous upper bound for this problem --
  even for the special case of log-concave distributions -- was doubly
  exponential in $1/\epsilon$ (and follows via the naive reduction to agnostic
  learning). Our approach relies on a novel computationally efficient procedure
  to certify whether a candidate solution is near-optimal, based on
  semi-definite programming. We use this certificate procedure as a black-box
  and turn it into an efficient learning algorithm by searching over the space
  of halfspaces via online convex optimization.</p>
  

</span>
  <!-- <span class="links"> -->
  <!--  -->
  <!--   [<a class="abstract">Abstract</a>] -->
  <!--  -->
  <!--  -->
  <!--   [<a href="http://arxiv.org/abs/2006.06467"
    target="_blank">arXiv</a>] -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- </span> -->

  <!-- <!-1- Hidden abstract block -1-> -->
  <!--  -->
  <!-- <span class="abstract hidden"> -->
  <!--   <p>We study the efficient PAC
  learnability of halfspaces in the presence of Tsybakov noise. In the Tsybakov
  noise model, each label is independently flipped with some probability which
  is controlled by an adversary. This noise model significantly generalizes the
  Massart noise model, by allowing the flipping probabilities to be arbitrarily
  close to $1/2$ for a fraction of the samples. Our main result is the first
  non-trivial PAC learning algorithm for this problem under a broad family of
  structured distributions -- satisfying certain concentration and
  (anti-)anti-concentration properties -- including log-concave distributions.
  Specifically, we given an algorithm that achieves misclassification error
  $\epsilon$ with respect to the true halfspace, with quasi-polynomial runtime
  dependence in $1/\epsilon$. The only previous upper bound for this problem --
  even for the special case of log-concave distributions -- was doubly
  exponential in $1/\epsilon$ (and follows via the naive reduction to agnostic
  learning). Our approach relies on a novel computationally efficient procedure
  to certify whether a candidate solution is near-optimal, based on
  semi-definite programming. We use this certificate procedure as a black-box
  and turn it into an efficient learning algorithm by searching over the space
  of halfspaces via online convex optimization.</p> -->
  <!-- </span> -->
  <!--  -->
</div>
</li>
<li>

<div id="DKZ20">
  
    <span class="title">Near-Optimal SQ Lower Bounds for Agnostically Learning Halfspaces and ReLUs under Gaussian Marginals <span class="nav">
    
    [<a data-toggle="collapse" href="#DKZ20-abstract">abstract</a>]
    
     
    [<a href="http://arxiv.org/abs/2006.16200" target="_blank">arxiv</a>] 
    
    
    
  </span>
    </span>
    <span class="author">
      
      
       
       
       
          
            
              
                <a href="http://www.iliasdiakonikolas.org" target="_blank">  Ilias Diakonikolas</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="https://cseweb.ucsd.edu/~dakane" target="_blank">  Daniel M. Kane</a>, 
              
            
          
        
      
      
       
       
       
          
            
              and <em>Nikos Zarifis</em>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Advances in Neural Information Processing Systems (NeurIPS 2020)</em>
    
 <!---   </span>
    <span class="notes"> -->
    
      
      </span>
  

<span id="DKZ20-materials">
 
    <!-- <li><a data-toggle="collapse" href="javascript:toggleDiv('DKZ20-bibtex')">BibTeX</a></li> -->
    <!-- <div id="DKZ20-bibtex" style="display:none;"> -->
    <!-- <pre class="collapse">@inproceedings{DKZ20,
  author = {Diakonikolas, Ilias and Kane, Daniel M. and Zarifis, Nikos},
  title = {Near-Optimal SQ Lower Bounds for Agnostically Learning Halfspaces and ReLUs under Gaussian Marginals},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS 2020)},
  arxiv = {2006.16200},
  year = {2020}
}
</pre> -->
    <!-- </div> -->


  
  <p id="DKZ20-abstract" class="collapse" style="border: 1px dashed
         black;padding:2px;
  font-size:11px">We study the fundamental problems of agnostically learning
             halfspaces and ReLUs under Gaussian marginals. In the former
             problem, given labeled examples $(\mathbf{x}, y)$ from an unknown
             distribution on $\mathbb{R}^d \times \{ \pm 1\}$, whose marginal
             distribution on $\mathbf{x}$ is the standard Gaussian and the
             labels $y$ can be arbitrary, the goal is to output a hypothesis
             with 0-1 loss $\mathrm{OPT}+\epsilon$, where $\mathrm{OPT}$ is the
             0-1 loss of the best-fitting halfspace. In the latter problem,
             given labeled examples $(\mathbf{x}, y)$ from an unknown
             distribution on $\mathbb{R}^d \times \mathbb{R}$, whose marginal
             distribution on $\mathbf{x}$ is the standard Gaussian and the
             labels $y$ can be arbitrary, the goal is to output a hypothesis
             with square loss $\mathrm{OPT}+\epsilon$, where $\mathrm{OPT}$ is
             the square loss of the best-fitting ReLU. We prove Statistical
             Query (SQ) lower bounds of $d^{\mathrm{poly}(1/\epsilon)}$ for both
             of these problems. Our SQ lower bounds provide strong evidence that
             current upper bounds for these tasks are essentially best
             possible.</p>
  

</span>
  <!-- <span class="links"> -->
  <!--  -->
  <!--   [<a class="abstract">Abstract</a>] -->
  <!--  -->
  <!--  -->
  <!--   [<a href="http://arxiv.org/abs/2006.16200"
    target="_blank">arXiv</a>] -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- </span> -->

  <!-- <!-1- Hidden abstract block -1-> -->
  <!--  -->
  <!-- <span class="abstract hidden"> -->
  <!--   <p>We study the fundamental problems of agnostically learning
             halfspaces and ReLUs under Gaussian marginals. In the former
             problem, given labeled examples $(\mathbf{x}, y)$ from an unknown
             distribution on $\mathbb{R}^d \times \{ \pm 1\}$, whose marginal
             distribution on $\mathbf{x}$ is the standard Gaussian and the
             labels $y$ can be arbitrary, the goal is to output a hypothesis
             with 0-1 loss $\mathrm{OPT}+\epsilon$, where $\mathrm{OPT}$ is the
             0-1 loss of the best-fitting halfspace. In the latter problem,
             given labeled examples $(\mathbf{x}, y)$ from an unknown
             distribution on $\mathbb{R}^d \times \mathbb{R}$, whose marginal
             distribution on $\mathbf{x}$ is the standard Gaussian and the
             labels $y$ can be arbitrary, the goal is to output a hypothesis
             with square loss $\mathrm{OPT}+\epsilon$, where $\mathrm{OPT}$ is
             the square loss of the best-fitting ReLU. We prove Statistical
             Query (SQ) lower bounds of $d^{\mathrm{poly}(1/\epsilon)}$ for both
             of these problems. Our SQ lower bounds provide strong evidence that
             current upper bounds for these tasks are essentially best
             possible.</p> -->
  <!-- </span> -->
  <!--  -->
</div>
</li>
<li>

<div id="DKTZ20c">
  
    <span class="title">Non-Convex SGD Learns Halfspaces with Adversarial Label Noise <span class="nav">
    
    [<a data-toggle="collapse" href="#DKTZ20c-abstract">abstract</a>]
    
     
    [<a href="http://arxiv.org/abs/2006.06742" target="_blank">arxiv</a>] 
    
    
    
  </span>
    </span>
    <span class="author">
      
      
       
       
       
          
            
              
                <a href="http://www.iliasdiakonikolas.org" target="_blank">  Ilias Diakonikolas</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="http://vkonton.github.io" target="_blank">  Vasilis Kontonis</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="http://www.tzamos.com" target="_blank">  Christos Tzamos</a>, 
              
            
          
        
      
      
       
       
       
          
            
              and <em>Nikos Zarifis</em>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Advances in Neural Information Processing Systems (NeurIPS 2020)</em>
    
 <!---   </span>
    <span class="notes"> -->
    
      
      </span>
  

<span id="DKTZ20c-materials">
 
    <!-- <li><a data-toggle="collapse" href="javascript:toggleDiv('DKTZ20c-bibtex')">BibTeX</a></li> -->
    <!-- <div id="DKTZ20c-bibtex" style="display:none;"> -->
    <!-- <pre class="collapse">@inproceedings{DKTZ20c,
  title = {Non-Convex SGD Learns Halfspaces with Adversarial Label Noise},
  author = {Diakonikolas, Ilias and Kontonis, Vasilis and Tzamos, Christos and Zarifis, Nikos},
  year = {2020},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS 2020)},
  arxiv = {2006.06742},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG}
}
</pre> -->
    <!-- </div> -->


  
  <p id="DKTZ20c-abstract" class="collapse" style="border: 1px dashed
         black;padding:2px;
  font-size:11px">We study the problem of agnostically learning homogeneous halfspaces in the distribution-specific PAC model. For a broad family of structured distributions, including log-concave distributions, we show that non-convex SGD efficiently converges to a solution with misclassification error $O(\opt)+\eps$, where $\opt$ is the misclassification error of the best-fitting halfspace. In sharp contrast, we show that optimizing any convex surrogate inherently leads to misclassification error of $\omega(\opt)$, even under Gaussian marginals.</p>
  

</span>
  <!-- <span class="links"> -->
  <!--  -->
  <!--   [<a class="abstract">Abstract</a>] -->
  <!--  -->
  <!--  -->
  <!--   [<a href="http://arxiv.org/abs/2006.06742"
    target="_blank">arXiv</a>] -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- </span> -->

  <!-- <!-1- Hidden abstract block -1-> -->
  <!--  -->
  <!-- <span class="abstract hidden"> -->
  <!--   <p>We study the problem of agnostically learning homogeneous halfspaces in the distribution-specific PAC model. For a broad family of structured distributions, including log-concave distributions, we show that non-convex SGD efficiently converges to a solution with misclassification error $O(\opt)+\eps$, where $\opt$ is the misclassification error of the best-fitting halfspace. In sharp contrast, we show that optimizing any convex surrogate inherently leads to misclassification error of $\omega(\opt)$, even under Gaussian marginals.</p> -->
  <!-- </span> -->
  <!--  -->
</div>
</li>
<li>

<div id="DKKZ20">
  
    <span class="title">Algorithms and SQ Lower Bounds for PAC Learning
	One-Hidden-Layer ReLU Networks <span class="nav">
    
    [<a data-toggle="collapse" href="#DKKZ20-abstract">abstract</a>]
    
     
    [<a href="http://arxiv.org/abs/2006.12476" target="_blank">arxiv</a>] 
    
    
    
  </span>
    </span>
    <span class="author">
      
      
       
       
       
          
            
              
                <a href="http://www.iliasdiakonikolas.org" target="_blank">  Ilias Diakonikolas</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="https://cseweb.ucsd.edu/~dakane" target="_blank">  Daniel M. Kane</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="http://vkonton.github.io" target="_blank">  Vasilis Kontonis</a>, 
              
            
          
        
      
      
       
       
       
          
            
              and <em>Nikos Zarifis</em>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the 33rd Annual Conference on Learning Theory (COLT 2020)</em>
    
 <!---   </span>
    <span class="notes"> -->
    
      
      </span>
  

<span id="DKKZ20-materials">
 
    <!-- <li><a data-toggle="collapse" href="javascript:toggleDiv('DKKZ20-bibtex')">BibTeX</a></li> -->
    <!-- <div id="DKKZ20-bibtex" style="display:none;"> -->
    <!-- <pre class="collapse">@inproceedings{DKKZ20,
  author = {Diakonikolas, Ilias and Kane, Daniel M. and Kontonis, Vasilis and Zarifis, Nikos},
  title = {Algorithms and SQ Lower Bounds for PAC Learning
  	One-Hidden-Layer ReLU Networks},
  booktitle = {Proceedings of the 33rd Annual Conference on Learning Theory (COLT 2020)},
  year = {2020},
  arxiv = {2006.12476}
}
</pre> -->
    <!-- </div> -->


  
  <p id="DKKZ20-abstract" class="collapse" style="border: 1px dashed
         black;padding:2px;
  font-size:11px">We study the problem of PAC learning one-hidden-layer ReLU networks
with $k$ hidden units
on $\R^d$ under Gaussian marginals in the presence of additive label noise. 
For the case of positive coefficients, we give the first polynomial-time algorithm 
for this learning problem for $k$ up to $\tilde{\Omega}(\sqrt{\log d})$. 
Previously, no polynomial time algorithm was known, even for $k=3$.
This answers an open question posed by~\cite{Kliv17}. Importantly,
our algorithm does not require any assumptions about the rank of the weight matrix
and its complexity is independent of its condition number. On the negative side,
for the more general task of PAC learning one-hidden-layer ReLU networks with positive or negative coefficients, 
we prove a Statistical Query lower bound of $d^{\Omega(k)}$. Thus, we provide a 
separation between the two classes in terms of efficient learnability.
Our upper and lower bounds are general, extending to broader families of activation functions.

             </p>
  

</span>
  <!-- <span class="links"> -->
  <!--  -->
  <!--   [<a class="abstract">Abstract</a>] -->
  <!--  -->
  <!--  -->
  <!--   [<a href="http://arxiv.org/abs/2006.12476"
    target="_blank">arXiv</a>] -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- </span> -->

  <!-- <!-1- Hidden abstract block -1-> -->
  <!--  -->
  <!-- <span class="abstract hidden"> -->
  <!--   <p>We study the problem of PAC learning one-hidden-layer ReLU networks
with $k$ hidden units
on $\R^d$ under Gaussian marginals in the presence of additive label noise. 
For the case of positive coefficients, we give the first polynomial-time algorithm 
for this learning problem for $k$ up to $\tilde{\Omega}(\sqrt{\log d})$. 
Previously, no polynomial time algorithm was known, even for $k=3$.
This answers an open question posed by~\cite{Kliv17}. Importantly,
our algorithm does not require any assumptions about the rank of the weight matrix
and its complexity is independent of its condition number. On the negative side,
for the more general task of PAC learning one-hidden-layer ReLU networks with positive or negative coefficients, 
we prove a Statistical Query lower bound of $d^{\Omega(k)}$. Thus, we provide a 
separation between the two classes in terms of efficient learnability.
Our upper and lower bounds are general, extending to broader families of activation functions.

             </p> -->
  <!-- </span> -->
  <!--  -->
</div>
</li>
<li>

<div id="DKTZ20a">
  
    <span class="title">Learning Halfspaces with Massart Noise Under Structured Distributions <span class="nav">
    
    [<a data-toggle="collapse" href="#DKTZ20a-abstract">abstract</a>]
    
     
    [<a href="http://arxiv.org/abs/2002.05632" target="_blank">arxiv</a>] 
    
    
    
  </span>
    </span>
    <span class="author">
      
      
       
       
       
          
            
              
                <a href="http://www.iliasdiakonikolas.org" target="_blank">  Ilias Diakonikolas</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="http://vkonton.github.io" target="_blank">  Vasilis Kontonis</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="http://www.tzamos.com" target="_blank">  Christos Tzamos</a>, 
              
            
          
        
      
      
       
       
       
          
            
              and <em>Nikos Zarifis</em>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the 33rd Annual Conference on Learning Theory (COLT 2020)</em>
    
 <!---   </span>
    <span class="notes"> -->
    
      
      </span>
  

<span id="DKTZ20a-materials">
 
    <!-- <li><a data-toggle="collapse" href="javascript:toggleDiv('DKTZ20a-bibtex')">BibTeX</a></li> -->
    <!-- <div id="DKTZ20a-bibtex" style="display:none;"> -->
    <!-- <pre class="collapse">@inproceedings{DKTZ20a,
  title = {Learning Halfspaces with Massart Noise Under Structured Distributions},
  author = {Diakonikolas, Ilias and Kontonis, Vasilis and Tzamos, Christos and Zarifis, Nikos},
  year = {2020},
  booktitle = {Proceedings of the 33rd Annual Conference on Learning Theory (COLT 2020)},
  eprint = {2002.05632},
  arxiv = {2002.05632},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG}
}
</pre> -->
    <!-- </div> -->


  
  <p id="DKTZ20a-abstract" class="collapse" style="border: 1px dashed
         black;padding:2px;
  font-size:11px">We study the problem of learning
  halfspaces with Massart noise in the distribution-specific PAC model. We give
  the first computationally efficient algorithm for this problem with respect to
  a broad family of distributions, including log-concave distributions. This
  resolves an open question posed in a number of prior works. Our approach is
  extremely simple: We identify a smooth {\em non-convex} surrogate loss with
  the property that any approximate stationary point of this loss defines a
  halfspace that is close to the target halfspace. Given this structural result,
  we can use SGD to solve the underlying learning problem.</p>
  

</span>
  <!-- <span class="links"> -->
  <!--  -->
  <!--   [<a class="abstract">Abstract</a>] -->
  <!--  -->
  <!--  -->
  <!--   [<a href="http://arxiv.org/abs/2002.05632"
    target="_blank">arXiv</a>] -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- </span> -->

  <!-- <!-1- Hidden abstract block -1-> -->
  <!--  -->
  <!-- <span class="abstract hidden"> -->
  <!--   <p>We study the problem of learning
  halfspaces with Massart noise in the distribution-specific PAC model. We give
  the first computationally efficient algorithm for this problem with respect to
  a broad family of distributions, including log-concave distributions. This
  resolves an open question posed in a number of prior works. Our approach is
  extremely simple: We identify a smooth {\em non-convex} surrogate loss with
  the property that any approximate stationary point of this loss defines a
  halfspace that is close to the target halfspace. Given this structural result,
  we can use SGD to solve the underlying learning problem.</p> -->
  <!-- </span> -->
  <!--  -->
</div>
</li>
<li>

<div id="FKKLSZ19">
  
    <span class="title">Reallocating multiple facilities on the line <span class="nav">
    
    [<a data-toggle="collapse" href="#FKKLSZ19-abstract">abstract</a>]
    
     
    [<a href="http://arxiv.org/abs/1905.12379" target="_blank">arxiv</a>] 
    
    
    
  </span>
    </span>
    <span class="author">
      
      
       
       
       
          
            
              
                <a href="http://www.softlab.ntua.gr/~fotakis/" target="_blank">  Dimitris Fotakis</a>, 
              
            
          
        
      
      
       
       
       
          
            
              
              Loukas  Kavouras,
              
            
          
        
      
      
       
       
       
          
            
              
              Panagiotis  Kostopanagiotis,
              
            
          
        
      
      
       
       
       
          
            
              
              Philip  Lazos,
              
            
          
        
      
      
       
       
       
          
            
              
                <a href="http://www.corelab.ntua.gr/~sskoul/" target="_blank">  Stratis Skoulakis</a>, 
              
            
          
        
      
      
       
       
       
          
            
              and <em>Nikos Zarifis</em>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Theoretical Computer Science 2021</em>
    
 <!---   </span>
    <span class="notes"> -->
    
      
      </span>
  

<span id="FKKLSZ19-materials">
 
    <!-- <li><a data-toggle="collapse" href="javascript:toggleDiv('FKKLSZ19-bibtex')">BibTeX</a></li> -->
    <!-- <div id="FKKLSZ19-bibtex" style="display:none;"> -->
    <!-- <pre class="collapse">@inproceedings{FKKLSZ19,
  title = {Reallocating multiple facilities on the line},
  author = {Fotakis, Dimitris and Kavouras, Loukas and Kostopanagiotis, Panagiotis and Lazos, Philip and Skoulakis, Stratis and Zarifis, Nikos},
  booktitle = {Theoretical Computer Science 2021},
  journal = {arXiv preprint},
  arxiv = {1905.12379},
  year = {2019}
}
</pre> -->
    <!-- </div> -->


  
  <p id="FKKLSZ19-abstract" class="collapse" style="border: 1px dashed
         black;padding:2px;
  font-size:11px">We study the multistage $K$-facility reallocation problem on the real line, where we maintain $K$ facility locations over $T$ stages, based on the stage-dependent locations of $n$ agents. Each agent is connected to the nearest facility at each stage, and the facilities may move from one stage to another, to accommodate different agent locations. The objective is to minimize the connection cost of the agents plus the total moving cost of the facilities, over all stages. $K$-facility reallocation was introduced by de Keijzer and Wojtczak, where they mostly focused on the special case of a single facility. Using an LP-based approach, we present a polynomial time algorithm that computes the optimal solution for any number of facilities. We also consider online $K$-facility reallocation, where the algorithm becomes aware of agent locations in a stage-by-stage fashion. By exploiting an interesting connection to the classical $K$-server problem, we present a constant-competitive algorithm for $K = 2$ facilities.</p>
  

</span>
  <!-- <span class="links"> -->
  <!--  -->
  <!--   [<a class="abstract">Abstract</a>] -->
  <!--  -->
  <!--  -->
  <!--   [<a href="http://arxiv.org/abs/1905.12379"
    target="_blank">arXiv</a>] -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- </span> -->

  <!-- <!-1- Hidden abstract block -1-> -->
  <!--  -->
  <!-- <span class="abstract hidden"> -->
  <!--   <p>We study the multistage $K$-facility reallocation problem on the real line, where we maintain $K$ facility locations over $T$ stages, based on the stage-dependent locations of $n$ agents. Each agent is connected to the nearest facility at each stage, and the facilities may move from one stage to another, to accommodate different agent locations. The objective is to minimize the connection cost of the agents plus the total moving cost of the facilities, over all stages. $K$-facility reallocation was introduced by de Keijzer and Wojtczak, where they mostly focused on the special case of a single facility. Using an LP-based approach, we present a polynomial time algorithm that computes the optimal solution for any number of facilities. We also consider online $K$-facility reallocation, where the algorithm becomes aware of agent locations in a stage-by-stage fashion. By exploiting an interesting connection to the classical $K$-server problem, we present a constant-competitive algorithm for $K = 2$ facilities.</p> -->
  <!-- </span> -->
  <!--  -->
</div>
</li></ol>


    </section>
    <!-- <footer> -->
    <!--    -->
    <!--   <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p> -->
    <!-- </footer> -->
  </div>
  <script src="/assets/js/scale.fix.js"></script>
  
</body>

</html>
